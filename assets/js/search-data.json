{
  
    
        "post0": {
            "title": "Exploring Titanic Dataset",
            "content": ". The objective of this notebook is to explain each steps and decision we take during solution and development of Titanic Dataset in Kaggle Competitions. . # data analysis import pandas as pd import numpy as np import random as rnd # data visualization import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # machine learning from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import Perceptron from sklearn.linear_model import SGDClassifier from sklearn.tree import DecisionTreeClassifier . . pd.options.display.max_columns = 100 . . 1.Reading data . The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together. . train = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) df = [train, test] . 2.Exploratory Data Analysis . Here we have to analyze and investigate data sets and summarize their main characteristics. . print(train.columns.values) . [&#39;PassengerId&#39; &#39;Survived&#39; &#39;Pclass&#39; &#39;Name&#39; &#39;Sex&#39; &#39;Age&#39; &#39;SibSp&#39; &#39;Parch&#39; &#39;Ticket&#39; &#39;Fare&#39; &#39;Cabin&#39; &#39;Embarked&#39;] . train.dtypes . PassengerId int64 Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype: object . We can classify data into : 1.Categorical and Numerical . Categorical: Survived, Sex, and Embarked. Ordinal: Pclass. | Numerical: Age, Fare. Discrete: SibSp, Parch. | . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . train.tail() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . Name contains Titles(eg. Mr,Mrs,Miss etc) | Ticket coloumn contains alphanumeric data. | Cabin is also alphanumeric. | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . In training dataset Cabin,Age,Embarked features contain a number of null values. | In testing dataset Cabin,Age contain a number of null values. | . ((train.isnull().sum()/len(train))*100) . PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 19.865320 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 77.104377 Embarked 0.224467 dtype: float64 . ((test.isnull().sum()/len(test))*100) . PassengerId 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 20.574163 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.239234 Cabin 78.229665 Embarked 0.000000 dtype: float64 . train.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . test.describe() . PassengerId Pclass Age SibSp Parch Fare . count 418.000000 | 418.000000 | 332.000000 | 418.000000 | 418.000000 | 417.000000 | . mean 1100.500000 | 2.265550 | 30.272590 | 0.447368 | 0.392344 | 35.627188 | . std 120.810458 | 0.841838 | 14.181209 | 0.896760 | 0.981429 | 55.907576 | . min 892.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | . 25% 996.250000 | 1.000000 | 21.000000 | 0.000000 | 0.000000 | 7.895800 | . 50% 1100.500000 | 3.000000 | 27.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 1204.750000 | 3.000000 | 39.000000 | 1.000000 | 0.000000 | 31.500000 | . max 1309.000000 | 3.000000 | 76.000000 | 8.000000 | 9.000000 | 512.329200 | . train.describe(include=[&#39;O&#39;]) . Name Sex Ticket Cabin Embarked . count 891 | 891 | 891 | 204 | 889 | . unique 891 | 2 | 681 | 147 | 3 | . top Foreman, Mr. Benjamin Laventall | male | 347082 | B96 B98 | S | . freq 1 | 577 | 7 | 4 | 644 | . test.describe(include=[&#39;O&#39;]) . Name Sex Ticket Cabin Embarked . count 418 | 418 | 418 | 91 | 418 | . unique 418 | 2 | 363 | 76 | 3 | . top Carlsson, Mr. Carl Robert | male | PC 17608 | B57 B59 B63 B66 | S | . freq 1 | 266 | 5 | 3 | 270 | . Names are unique,Total 891 unique names, | 65% of data are male. | Cabin values have several dupicates. Meaning lot of people shared a cabin. | Embarked takes three possible values. | S port used by most passengers. //Southampton | Ticket feature has 22% of duplicate values | . train[[&#39;Pclass&#39;, &#39;Survived&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Pclass Survived . 0 1 | 0.629630 | . 1 2 | 0.472826 | . 2 3 | 0.242363 | . train[[&quot;Sex&quot;, &quot;Survived&quot;]].groupby([&#39;Sex&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Sex Survived . 0 female | 0.742038 | . 1 male | 0.188908 | . train[[&quot;SibSp&quot;, &quot;Survived&quot;]].groupby([&#39;SibSp&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . SibSp Survived . 1 1 | 0.535885 | . 2 2 | 0.464286 | . 0 0 | 0.345395 | . 3 3 | 0.250000 | . 4 4 | 0.166667 | . 5 5 | 0.000000 | . 6 8 | 0.000000 | . train[[&quot;Parch&quot;, &quot;Survived&quot;]].groupby([&#39;Parch&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Parch Survived . 3 3 | 0.600000 | . 1 1 | 0.550847 | . 2 2 | 0.500000 | . 0 0 | 0.343658 | . 5 5 | 0.200000 | . 4 4 | 0.000000 | . 6 6 | 0.000000 | . Pclass We observe significant correlation (&gt;0.5) among Pclass=1 and Survived. | Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74%. | SibSp and Parch These features have zero correlation for certain values. | . 3.Data Visualization . Visualization of data can reveal many insights that can help us in determining features in modelling. . sns.set(palette = &quot;flare&quot;) grid = sns.FacetGrid(train, col=&#39;Survived&#39;,height = 10) grid.map(plt.hist, &#39;Age&#39;, bins=20) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4706713280&gt; . Most passengers are in 15-35 age range. | Children aged &lt;=4 had a high survival rate. | Oldest passengers aged above 80 survived. | Large number of 15-25 year olds did not survive. | Age is an important feature. | . grid = sns.FacetGrid(train, col=&#39;Survived&#39;, row=&#39;Pclass&#39;, size=5) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend(); . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . Pclass=3 had most passengers, however most did not survive. | Child passengers in Pclass=2 and Pclass=3 mostly survived. | Most passengers in Pclass=1 survived. | Pclass varies in terms of Age distribution of passengers. | . grid = sns.FacetGrid(train, row=&#39;Embarked&#39;, size=5) grid.map(sns.pointplot, &#39;Pclass&#39;, &#39;Survived&#39;, &#39;Sex&#39;) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:643: UserWarning: Using the pointplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:648: UserWarning: Using the pointplot function without specifying `hue_order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4703a897f0&gt; . Female passengers had much better survival rate than males. | In Embarked=C where males had higher survival rate. | Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. | Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. | . grid = sns.FacetGrid(train, row=&#39;Embarked&#39;, col=&#39;Survived&#39;, size=5) grid.map(sns.barplot, &#39;Sex&#39;, &#39;Fare&#39;, alpha=.5, ci=None) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:643: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4703a3faf0&gt; . Higher fare paying passengers had better survival. | Port of embarkation correlates with survival rates. | . 4.Feature Engineering . for dataset in df: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;, expand=False) pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in df: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Lady&#39;, &#39;Countess&#39;,&#39;Capt&#39;, &#39;Col&#39;, &#39;Don&#39;, &#39;Dr&#39;, &#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;, &#39;Jonkheer&#39;, &#39;Dona&#39;], &#39;Other&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mlle&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Ms&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby([&#39;Title&#39;], as_index=False).mean() . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Other | 0.347826 | . title_mapping = {&quot;Mr&quot;: 1, &quot;Miss&quot;: 2, &quot;Mrs&quot;: 3, &quot;Master&quot;: 4, &quot;Other&quot;: 5} for dataset in df: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].map(title_mapping) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].fillna(0) train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . train = train.drop([&#39;Name&#39;, &#39;PassengerId&#39;], axis=1) test = test.drop([&#39;Name&#39;], axis=1) df = [train, test] train.shape, test.shape #Dropping the Name and PassengerID feature from training and testing datasets . ((891, 11), (418, 11)) . Converting Categorical Features . for dataset in df: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].map( {&#39;female&#39;: 1, &#39;male&#39;: 0} ).astype(int) train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 0 | 3 | 0 | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 1 | 1 | 1 | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 1 | 3 | 1 | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 1 | 1 | 1 | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 0 | 3 | 0 | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . grid = sns.FacetGrid(train, row=&#39;Pclass&#39;, col=&#39;Sex&#39;, size=5) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4706753550&gt; . for dataset in df: dataset[&quot;Age&quot;].fillna(dataset.groupby(&quot;Title&quot;)[&quot;Age&quot;].transform(&quot;median&quot;), inplace=True) train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 Title 0 dtype: int64 . train[&#39;AgeBand&#39;] = pd.cut(train[&#39;Age&#39;], 5) train[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;],as_index=False).mean().sort_values(by=&#39;AgeBand&#39;, ascending=True) . AgeBand Survived . 0 (0.34, 16.336] | 0.548077 | . 1 (16.336, 32.252] | 0.327345 | . 2 (32.252, 48.168] | 0.439024 | . 3 (48.168, 64.084] | 0.428571 | . 4 (64.084, 80.0] | 0.090909 | . Age values should be replaced as ordinals. . for dataset in df: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title AgeBand . 0 0 | 3 | 0 | 1.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | (16.336, 32.252] | . 1 1 | 1 | 1 | 2.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | (32.252, 48.168] | . 2 1 | 3 | 1 | 1.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | (16.336, 32.252] | . 3 1 | 1 | 1 | 2.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | (32.252, 48.168] | . 4 0 | 3 | 0 | 2.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | (32.252, 48.168] | . train = train.drop([&#39;AgeBand&#39;], axis=1) #removing AgeBand df = [train, test] train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 0 | 3 | 0 | 1.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 1 | 1 | 1 | 2.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 1 | 3 | 1 | 1.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 1 | 1 | 1 | 2.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 0 | 3 | 0 | 2.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . for dataset in df: dataset[&#39;FamilySize&#39;] = dataset[&#39;SibSp&#39;] + dataset[&#39;Parch&#39;] + 1 train[[&#39;FamilySize&#39;, &#39;Survived&#39;]].groupby([&#39;FamilySize&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . FamilySize Survived . 3 4 | 0.724138 | . 2 3 | 0.578431 | . 1 2 | 0.552795 | . 6 7 | 0.333333 | . 0 1 | 0.303538 | . 4 5 | 0.200000 | . 5 6 | 0.136364 | . 7 8 | 0.000000 | . 8 11 | 0.000000 | . for dataset in df: dataset[&#39;Alone&#39;] = 0 dataset.loc[dataset[&#39;FamilySize&#39;] == 1, &#39;Alone&#39;] = 1 train[[&#39;Alone&#39;, &#39;Survived&#39;]].groupby([&#39;Alone&#39;], as_index=False).mean() . Alone Survived . 0 0 | 0.505650 | . 1 1 | 0.303538 | . train = train.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;], axis=1) test = test.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;], axis=1) df = [train, test] train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 7.2500 | S | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 71.2833 | C | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 7.9250 | S | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 53.1000 | S | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 8.0500 | S | 1 | 1 | . train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 Fare 0 Embarked 2 Title 0 Alone 0 dtype: int64 . mode = train.Embarked.dropna().mode()[0] for dataset in df: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(mode) train[[&#39;Embarked&#39;, &#39;Survived&#39;]].groupby([&#39;Embarked&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Embarked Survived . 0 C | 0.553571 | . 1 Q | 0.389610 | . 2 S | 0.339009 | . Converting categorical values to numeric values . for dataset in df: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2} ).astype(int) train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 7.2500 | 0 | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 71.2833 | 1 | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 7.9250 | 0 | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 53.1000 | 0 | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 8.0500 | 0 | 1 | 1 | . train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 Fare 0 Embarked 0 Title 0 Alone 0 dtype: int64 . test.isnull().sum() . PassengerId 0 Pclass 0 Sex 0 Age 0 Fare 1 Embarked 0 Title 0 Alone 0 dtype: int64 . test[&#39;Fare&#39;].fillna(test[&#39;Fare&#39;].dropna().median(), inplace=True) test.head() . PassengerId Pclass Sex Age Fare Embarked Title Alone . 0 892 | 3 | 0 | 2.0 | 7.8292 | 2 | 1 | 1 | . 1 893 | 3 | 1 | 2.0 | 7.0000 | 0 | 3 | 0 | . 2 894 | 2 | 0 | 3.0 | 9.6875 | 2 | 1 | 1 | . 3 895 | 3 | 0 | 1.0 | 8.6625 | 0 | 1 | 1 | . 4 896 | 3 | 1 | 1.0 | 12.2875 | 0 | 3 | 0 | . train[&#39;FareBand&#39;] = pd.qcut(train[&#39;Fare&#39;], 4) train[[&#39;FareBand&#39;, &#39;Survived&#39;]].groupby([&#39;FareBand&#39;], as_index=False).mean().sort_values(by=&#39;FareBand&#39;, ascending=True) . FareBand Survived . 0 (-0.001, 7.91] | 0.197309 | . 1 (7.91, 14.454] | 0.303571 | . 2 (14.454, 31.0] | 0.454955 | . 3 (31.0, 512.329] | 0.581081 | . for dataset in df: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.91, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.91) &amp; (dataset[&#39;Fare&#39;] &lt;= 14.454), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 14.454) &amp; (dataset[&#39;Fare&#39;] &lt;= 31), &#39;Fare&#39;] = 2 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 31, &#39;Fare&#39;] = 3 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].astype(int) train = train.drop([&#39;FareBand&#39;], axis=1) df = [train, test] train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 0 | 0 | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 3 | 1 | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 1 | 0 | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 3 | 0 | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 1 | 0 | 1 | 1 | . test.head() . PassengerId Pclass Sex Age Fare Embarked Title Alone . 0 892 | 3 | 0 | 2.0 | 0 | 2 | 1 | 1 | . 1 893 | 3 | 1 | 2.0 | 0 | 0 | 3 | 0 | . 2 894 | 2 | 0 | 3.0 | 1 | 2 | 1 | 1 | . 3 895 | 3 | 0 | 1.0 | 1 | 0 | 1 | 1 | . 4 896 | 3 | 1 | 1.0 | 1 | 0 | 3 | 0 | . Modelling . Now we can train a model and predict the required solution.Best Model should be selected from these ML algorithms. . Logistic Regression | KNN or k-Nearest Neighbors | Support Vector Machines | Naive Bayes classifier | Decision Tree | Random Forrest | . X_train = train.drop(&quot;Survived&quot;, axis=1) Y_train = train[&quot;Survived&quot;] X_test = test.drop(&quot;PassengerId&quot;, axis=1).copy() X_train.shape, Y_train.shape, X_test.shape . ((891, 7), (891,), (418, 7)) . logreg = LogisticRegression() logreg.fit(X_train, Y_train) Y_pred = logreg.predict(X_test) acc_log = round(logreg.score(X_train, Y_train) * 100, 2) acc_log . 78.56 . coeff_df = pd.DataFrame(train.columns.delete(0)) coeff_df.columns = [&#39;Feature&#39;] coeff_df[&quot;Correlation&quot;] = pd.Series(logreg.coef_[0]) coeff_df.sort_values(by=&#39;Correlation&#39;, ascending=False) . Feature Correlation . 1 Sex | 2.148684 | . 5 Title | 0.413368 | . 4 Embarked | 0.313126 | . 6 Alone | 0.051407 | . 2 Age | -0.032112 | . 3 Fare | -0.037693 | . 0 Pclass | -0.996006 | . svc = SVC() svc.fit(X_train, Y_train) Y_pred = svc.predict(X_test) acc_svc = round(svc.score(X_train, Y_train) * 100, 2) acc_svc . 78.34 . knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train, Y_train) Y_pred = knn.predict(X_test) acc_knn = round(knn.score(X_train, Y_train) * 100, 2) acc_knn . 83.84 . gaussian = GaussianNB() gaussian.fit(X_train, Y_train) Y_pred = gaussian.predict(X_test) acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2) acc_gaussian . 76.99 . decision_tree = DecisionTreeClassifier() decision_tree.fit(X_train, Y_train) Y_pred = decision_tree.predict(X_test) acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2) acc_decision_tree . 87.21 . random_forest = RandomForestClassifier(n_estimators=100) random_forest.fit(X_train, Y_train) Y_pred = random_forest.predict(X_test) random_forest.score(X_train, Y_train) acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2) acc_random_forest . 87.21 . models = pd.DataFrame({ &#39;Model&#39;: [&#39;Support Vector Machines&#39;, &#39;KNN&#39;, &#39;Logistic Regression&#39;, &#39;Random Forest&#39;,&#39;Naive Bayes&#39;,&#39;Decision Tree&#39;], &#39;Score&#39;: [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_decision_tree]}) models.sort_values(by=&#39;Score&#39;, ascending=False) . Model Score . 3 Random Forest | 87.21 | . 5 Decision Tree | 87.21 | . 1 KNN | 83.84 | . 2 Logistic Regression | 78.56 | . 0 Support Vector Machines | 78.34 | . 4 Naive Bayes | 76.99 | . submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: Y_pred }) . Any suggestions to improve our score are most welcome. . . twitter: https://twitter.com/jithinharidaas/ .",
            "url": "https://jithinharidas.github.io/paranormal-distributions/2021/04/21/Exploring-Titanic-Dataset.html",
            "relUrl": "/2021/04/21/Exploring-Titanic-Dataset.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploring Iris Dataset",
            "content": ". Here we&#39;re exploring basics of classification with the iris data set. . The first step is to use the scikit-learn python package to import the preloaded data sets. . from sklearn.datasets import load_iris #data is saved as a variable iris = load_iris() #view data description and information print(iris.DESCR) . .. _iris_dataset: Iris plants dataset -- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&#39;s paper. Note that it&#39;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher&#39;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. &#34;The use of multiple measurements in taxonomic problems&#34; Annual Eugenics, 7, Part II, 179-188 (1936); also in &#34;Contributions to Mathematical Statistics&#34; (John Wiley, NY, 1950). - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) &#34;Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments&#34;. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) &#34;The Reduced Nearest Neighbor Rule&#34;. IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&#34;s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... . Putting Data into a Data Frame . import pandas as pd data = pd.DataFrame(iris.data) data.head() . 0 1 2 3 . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . renaming the columns for clarity. . data.columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;] data.head() . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . target = pd.DataFrame(iris.target) #rename the column to make it clear that these are the target values target = target.rename(columns = {0: &#39;target&#39;}) target.head() . target . 0 0 | . 1 0 | . 2 0 | . 3 0 | . 4 0 | . The target data frame is only one column, and it gives a list of the values 0, 1, and 2. We will use the information from the feature data to predict if a flower belongs in group 0, 1, or 2. . 0 is Iris Setosa | 1 is Iris Versicolour | 2 is Iris Virginica | . 1. Exploratory Data Analysis (EDA) . df = pd.concat([data, target], axis = 1) . df.head() . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . 1.1 Data Cleaning . It&#39;s critical to go over the data, make sure it&#39;s clean, and then start looking for patterns between characteristics and target variables. . df.dtypes . sepal_length float64 sepal_width float64 petal_length float64 petal_width float64 target int32 dtype: object . float = numbers with decimals | int = integer or whole number without decimals | obj = object, string, or words | . The data types in this data set are all ready to be modelled. . df.isnull().sum() . sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 dtype: int64 . This data set is not missing any values. . df.describe() . sepal_length sepal_width petal_length petal_width target . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | 1.000000 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | 0.819232 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | 0.000000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | 0.000000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | 1.000000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | 2.000000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | 2.000000 | . 2. Visualizing . import seaborn as sns sns.heatmap(df.corr(), annot = True) . &lt;AxesSubplot:&gt; . The target value is most correlated with the length and width of the petals, which means that as these numbers increase, so does the target value. | In this case, it signifies that flowers in class 2 have petal length and width that are generally longer and wider than flowers in class 0. | Sepal width is the most anti-correlated, implying that flowers in class 0 have the widest sepals compared to flowers in class 2. | also see some intercorrelation between features, for example petal width and length are also highly correlated. | . import matplotlib.pyplot as plt . we can plot scatter plots to further visualize the way the different classes of flowers relate to sepal and petal data. . x_index = 0 y_index = 1 # this formatter will label the colorbar with the correct target names formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)]) plt.figure(figsize=(5, 4)) plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target) plt.colorbar(ticks=[0, 1, 2], format=formatter) plt.xlabel(iris.feature_names[x_index]) plt.ylabel(iris.feature_names[y_index]) plt.tight_layout() plt.show() . Now let&#8217;s create the same scatter plot to compare the petal data points. . x_index = 2 y_index = 3 # this formatter will label the colorbar with the correct target names formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)]) plt.figure(figsize=(5, 4)) plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target) plt.colorbar(ticks=[0, 1, 2], format=formatter) plt.xlabel(iris.feature_names[x_index]) plt.ylabel(iris.feature_names[y_index]) plt.tight_layout() plt.show() . 3. Modeling . X = df.copy() y = X.pop(&#39;target&#39;) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=1, stratify = y) . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns) X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns) . 3.1 Baseline Prediction . The baseline is the probability of predicting class before the model is implemented. If the data is split into 2 classes evenly, there is already a 50% chance of randomly assigning an element to the correct class. The goal of our model is to improve on this baseline, or random prediction. Also, if there is a strong class imbalance (if 90% of the data was in class 1), then we could alter the proportion of each class to help the model predict more accurately. . df.target.value_counts(normalize= True) . 0 0.333333 1 0.333333 2 0.333333 Name: target, dtype: float64 . The baseline prediction for this model is 1/3 . 3.2 Logistic Regression Model . import numpy as np . from sklearn.linear_model import LogisticRegression #create the model instance model = LogisticRegression() #fit the model on the training data model.fit(X_train, y_train) #the score, or accuracy of the model model.score(X_test, y_test) . 0.9666666666666667 . from sklearn.model_selection import cross_val_score scores = cross_val_score(model, X_train, y_train, cv=10) print(np.mean(scores)) . 0.9499999999999998 . Without any adjustments or tuning, this model is already performing very well with a test score of .9667 and a cross validation score of .9499. This means that the model is predicting the correct class for the flower about 95% of time. Much higher than the baseline of 33%! . 4. Understanding the Predictions . df_coef = pd.DataFrame(model.coef_, columns=X_train.columns) df_coef . sepal_length sepal_width petal_length petal_width . 0 -1.102746 | 1.001818 | -1.836891 | -1.667978 | . 1 0.402982 | -0.323432 | -0.277761 | -0.650011 | . 2 0.699764 | -0.678386 | 2.114653 | 2.317989 | . Coefficients are often a bit hard to interpret in Logistic Regression, but we can get an idea of how much of an impact each of the features had in deciding if a flower belonged to that class. For instance, petal length was barely a deciding factor for if a flower was in class 1, but petal width was a strong predictor for class 2 . predictions = model.predict(X_test) #compare predicted values with the actual scores compare_df = pd.DataFrame({&#39;actual&#39;: y_test, &#39;predicted&#39;: predictions}) compare_df = compare_df.reset_index(drop = True) compare_df . actual predicted . 0 2 | 2 | . 1 0 | 0 | . 2 1 | 1 | . 3 0 | 0 | . 4 0 | 0 | . 5 0 | 0 | . 6 2 | 2 | . 7 2 | 2 | . 8 2 | 2 | . 9 1 | 1 | . 10 0 | 0 | . 11 1 | 1 | . 12 2 | 2 | . 13 1 | 1 | . 14 2 | 2 | . 15 0 | 0 | . 16 2 | 2 | . 17 1 | 1 | . 18 1 | 1 | . 19 2 | 2 | . 20 1 | 1 | . 21 1 | 1 | . 22 0 | 0 | . 23 0 | 0 | . 24 2 | 2 | . 25 2 | 1 | . 26 0 | 0 | . 27 0 | 0 | . 28 1 | 1 | . 29 1 | 1 | . The predictions line up almost perfectly, and only once the model incorrectly predicted that a flower belonged to class 1 when it really belonged to class 2. . from sklearn.metrics import confusion_matrix pd.DataFrame(confusion_matrix(y_test, predictions, labels=[2, 1, 0]),index=[2, 1, 0], columns=[2, 1, 0]) . 2 1 0 . 2 9 | 1 | 0 | . 1 0 | 10 | 0 | . 0 0 | 0 | 10 | . We can see that class 0 and 1 were all predicted correctly all 10 times, but the model incorrectly labeled class 2 as class 1 in one instance. . Precision: Number of correctly predicted Iris Virginica flowers (10) out of total number of predicted Iris Virginica flowers (10). Precision in predicting Iris Virginica =10/10 = 1.0 | Recall: Number of correctly predicted Iris Virginica out of the number of actual Iris Virginica. Recall = 9/10 = .9 | F1 Score: This is a harmonic mean of precision and recall. The formula is F1 Score = 2 (precision recall) / (precision + recall) | Accuracy: Add all the correct predictions together for all classes and divide by the total number of predictions. 29 correct predictions /30 total values = accuracy of .9667. | . from sklearn.metrics import classification_report print(classification_report(y_test, predictions)) . precision recall f1-score support 0 1.00 1.00 1.00 10 1 0.91 1.00 0.95 10 2 1.00 0.90 0.95 10 accuracy 0.97 30 macro avg 0.97 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 . probs = model.predict_proba(X_test) #put the probabilities into a dataframe for easier viewing Y_pp = pd.DataFrame(model.predict_proba(X_test), columns=[&#39;class_0_pp&#39;, &#39;class_1_pp&#39;, &#39;class_2_pp&#39;]) Y_pp.head() . class_0_pp class_1_pp class_2_pp . 0 0.000016 | 0.062182 | 9.378022e-01 | . 1 0.958819 | 0.041181 | 5.060799e-07 | . 2 0.147033 | 0.846368 | 6.598360e-03 | . 3 0.983033 | 0.016967 | 2.623772e-07 | . 4 0.970334 | 0.029663 | 2.118099e-06 | . 5. Conclusion . This is a typical data set since it is simple to work with, but the steps outlined here may be applied to any classification project. . . twitter: https://twitter.com/jithinharidaas/ .",
            "url": "https://jithinharidas.github.io/paranormal-distributions/2020/06/21/Exploring-Iris-Dataset.html",
            "relUrl": "/2020/06/21/Exploring-Iris-Dataset.html",
            "date": " • Jun 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Jithin K Haridas . . I specialise in Advanced Analytics, a combination of exploratory data analysis, prescriptive and predictive analytics leveraging techniques across areas like statistical modelling, data mining, machine learning, text analytics. . .",
          "url": "https://jithinharidas.github.io/paranormal-distributions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jithinharidas.github.io/paranormal-distributions/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}