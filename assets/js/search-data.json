{
  
    
        "post0": {
            "title": "Exploring Titanic Dataset",
            "content": ". The objective of this notebook is to explain each steps and decision we take during solution and development of Titanic Dataset in Kaggle Competitions. . # data analysis import pandas as pd import numpy as np import random as rnd # data visualization import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # machine learning from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import Perceptron from sklearn.linear_model import SGDClassifier from sklearn.tree import DecisionTreeClassifier . . pd.options.display.max_columns = 100 . . 1.Reading data . The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together. . train = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) df = [train, test] . 2.Exploratory Data Analysis . Here we have to analyze and investigate data sets and summarize their main characteristics. . print(train.columns.values) . [&#39;PassengerId&#39; &#39;Survived&#39; &#39;Pclass&#39; &#39;Name&#39; &#39;Sex&#39; &#39;Age&#39; &#39;SibSp&#39; &#39;Parch&#39; &#39;Ticket&#39; &#39;Fare&#39; &#39;Cabin&#39; &#39;Embarked&#39;] . train.dtypes . PassengerId int64 Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype: object . We can classify data into : 1.Categorical and Numerical . Categorical: Survived, Sex, and Embarked. Ordinal: Pclass. | Numerical: Age, Fare. Discrete: SibSp, Parch. | . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . train.tail() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . Name contains Titles(eg. Mr,Mrs,Miss etc) | Ticket coloumn contains alphanumeric data. | Cabin is also alphanumeric. | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . In training dataset Cabin,Age,Embarked features contain a number of null values. | In testing dataset Cabin,Age contain a number of null values. | . ((train.isnull().sum()/len(train))*100) . PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 19.865320 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 77.104377 Embarked 0.224467 dtype: float64 . ((test.isnull().sum()/len(test))*100) . PassengerId 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 20.574163 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.239234 Cabin 78.229665 Embarked 0.000000 dtype: float64 . train.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . test.describe() . PassengerId Pclass Age SibSp Parch Fare . count 418.000000 | 418.000000 | 332.000000 | 418.000000 | 418.000000 | 417.000000 | . mean 1100.500000 | 2.265550 | 30.272590 | 0.447368 | 0.392344 | 35.627188 | . std 120.810458 | 0.841838 | 14.181209 | 0.896760 | 0.981429 | 55.907576 | . min 892.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | . 25% 996.250000 | 1.000000 | 21.000000 | 0.000000 | 0.000000 | 7.895800 | . 50% 1100.500000 | 3.000000 | 27.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 1204.750000 | 3.000000 | 39.000000 | 1.000000 | 0.000000 | 31.500000 | . max 1309.000000 | 3.000000 | 76.000000 | 8.000000 | 9.000000 | 512.329200 | . train.describe(include=[&#39;O&#39;]) . Name Sex Ticket Cabin Embarked . count 891 | 891 | 891 | 204 | 889 | . unique 891 | 2 | 681 | 147 | 3 | . top Foreman, Mr. Benjamin Laventall | male | 347082 | B96 B98 | S | . freq 1 | 577 | 7 | 4 | 644 | . test.describe(include=[&#39;O&#39;]) . Name Sex Ticket Cabin Embarked . count 418 | 418 | 418 | 91 | 418 | . unique 418 | 2 | 363 | 76 | 3 | . top Carlsson, Mr. Carl Robert | male | PC 17608 | B57 B59 B63 B66 | S | . freq 1 | 266 | 5 | 3 | 270 | . Names are unique,Total 891 unique names, | 65% of data are male. | Cabin values have several dupicates. Meaning lot of people shared a cabin. | Embarked takes three possible values. | S port used by most passengers. //Southampton | Ticket feature has 22% of duplicate values | . train[[&#39;Pclass&#39;, &#39;Survived&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Pclass Survived . 0 1 | 0.629630 | . 1 2 | 0.472826 | . 2 3 | 0.242363 | . train[[&quot;Sex&quot;, &quot;Survived&quot;]].groupby([&#39;Sex&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Sex Survived . 0 female | 0.742038 | . 1 male | 0.188908 | . train[[&quot;SibSp&quot;, &quot;Survived&quot;]].groupby([&#39;SibSp&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . SibSp Survived . 1 1 | 0.535885 | . 2 2 | 0.464286 | . 0 0 | 0.345395 | . 3 3 | 0.250000 | . 4 4 | 0.166667 | . 5 5 | 0.000000 | . 6 8 | 0.000000 | . train[[&quot;Parch&quot;, &quot;Survived&quot;]].groupby([&#39;Parch&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Parch Survived . 3 3 | 0.600000 | . 1 1 | 0.550847 | . 2 2 | 0.500000 | . 0 0 | 0.343658 | . 5 5 | 0.200000 | . 4 4 | 0.000000 | . 6 6 | 0.000000 | . Pclass We observe significant correlation (&gt;0.5) among Pclass=1 and Survived. | Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74%. | SibSp and Parch These features have zero correlation for certain values. | . 3.Data Visualization . Visualization of data can reveal many insights that can help us in determining features in modelling. . sns.set(palette = &quot;flare&quot;) grid = sns.FacetGrid(train, col=&#39;Survived&#39;,height = 10) grid.map(plt.hist, &#39;Age&#39;, bins=20) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4706713280&gt; . Most passengers are in 15-35 age range. | Children aged &lt;=4 had a high survival rate. | Oldest passengers aged above 80 survived. | Large number of 15-25 year olds did not survive. | Age is an important feature. | . grid = sns.FacetGrid(train, col=&#39;Survived&#39;, row=&#39;Pclass&#39;, size=5) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend(); . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . Pclass=3 had most passengers, however most did not survive. | Child passengers in Pclass=2 and Pclass=3 mostly survived. | Most passengers in Pclass=1 survived. | Pclass varies in terms of Age distribution of passengers. | . grid = sns.FacetGrid(train, row=&#39;Embarked&#39;, size=5) grid.map(sns.pointplot, &#39;Pclass&#39;, &#39;Survived&#39;, &#39;Sex&#39;) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:643: UserWarning: Using the pointplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:648: UserWarning: Using the pointplot function without specifying `hue_order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4703a897f0&gt; . Female passengers had much better survival rate than males. | In Embarked=C where males had higher survival rate. | Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. | Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. | . grid = sns.FacetGrid(train, row=&#39;Embarked&#39;, col=&#39;Survived&#39;, size=5) grid.map(sns.barplot, &#39;Sex&#39;, &#39;Fare&#39;, alpha=.5, ci=None) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:643: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4703a3faf0&gt; . Higher fare paying passengers had better survival. | Port of embarkation correlates with survival rates. | . 4.Feature Engineering . for dataset in df: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;, expand=False) pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in df: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Lady&#39;, &#39;Countess&#39;,&#39;Capt&#39;, &#39;Col&#39;, &#39;Don&#39;, &#39;Dr&#39;, &#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;, &#39;Jonkheer&#39;, &#39;Dona&#39;], &#39;Other&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mlle&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Ms&#39;, &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby([&#39;Title&#39;], as_index=False).mean() . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Other | 0.347826 | . title_mapping = {&quot;Mr&quot;: 1, &quot;Miss&quot;: 2, &quot;Mrs&quot;: 3, &quot;Master&quot;: 4, &quot;Other&quot;: 5} for dataset in df: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].map(title_mapping) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].fillna(0) train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . train = train.drop([&#39;Name&#39;, &#39;PassengerId&#39;], axis=1) test = test.drop([&#39;Name&#39;], axis=1) df = [train, test] train.shape, test.shape #Dropping the Name and PassengerID feature from training and testing datasets . ((891, 11), (418, 11)) . Converting Categorical Features . for dataset in df: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].map( {&#39;female&#39;: 1, &#39;male&#39;: 0} ).astype(int) train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 0 | 3 | 0 | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 1 | 1 | 1 | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 1 | 3 | 1 | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 1 | 1 | 1 | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 0 | 3 | 0 | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . grid = sns.FacetGrid(train, row=&#39;Pclass&#39;, col=&#39;Sex&#39;, size=5) grid.map(plt.hist, &#39;Age&#39;, alpha=.5, bins=20) grid.add_legend() . /home/jithin/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:316: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f4706753550&gt; . for dataset in df: dataset[&quot;Age&quot;].fillna(dataset.groupby(&quot;Title&quot;)[&quot;Age&quot;].transform(&quot;median&quot;), inplace=True) train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 Title 0 dtype: int64 . train[&#39;AgeBand&#39;] = pd.cut(train[&#39;Age&#39;], 5) train[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;],as_index=False).mean().sort_values(by=&#39;AgeBand&#39;, ascending=True) . AgeBand Survived . 0 (0.34, 16.336] | 0.548077 | . 1 (16.336, 32.252] | 0.327345 | . 2 (32.252, 48.168] | 0.439024 | . 3 (48.168, 64.084] | 0.428571 | . 4 (64.084, 80.0] | 0.090909 | . Age values should be replaced as ordinals. . for dataset in df: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title AgeBand . 0 0 | 3 | 0 | 1.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | (16.336, 32.252] | . 1 1 | 1 | 1 | 2.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | (32.252, 48.168] | . 2 1 | 3 | 1 | 1.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | (16.336, 32.252] | . 3 1 | 1 | 1 | 2.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | (32.252, 48.168] | . 4 0 | 3 | 0 | 2.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | (32.252, 48.168] | . train = train.drop([&#39;AgeBand&#39;], axis=1) #removing AgeBand df = [train, test] train.head() . Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 0 | 3 | 0 | 1.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 1 | . 1 1 | 1 | 1 | 2.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 3 | . 2 1 | 3 | 1 | 1.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 2 | . 3 1 | 1 | 1 | 2.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 3 | . 4 0 | 3 | 0 | 2.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 1 | . for dataset in df: dataset[&#39;FamilySize&#39;] = dataset[&#39;SibSp&#39;] + dataset[&#39;Parch&#39;] + 1 train[[&#39;FamilySize&#39;, &#39;Survived&#39;]].groupby([&#39;FamilySize&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . FamilySize Survived . 3 4 | 0.724138 | . 2 3 | 0.578431 | . 1 2 | 0.552795 | . 6 7 | 0.333333 | . 0 1 | 0.303538 | . 4 5 | 0.200000 | . 5 6 | 0.136364 | . 7 8 | 0.000000 | . 8 11 | 0.000000 | . for dataset in df: dataset[&#39;Alone&#39;] = 0 dataset.loc[dataset[&#39;FamilySize&#39;] == 1, &#39;Alone&#39;] = 1 train[[&#39;Alone&#39;, &#39;Survived&#39;]].groupby([&#39;Alone&#39;], as_index=False).mean() . Alone Survived . 0 0 | 0.505650 | . 1 1 | 0.303538 | . train = train.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;], axis=1) test = test.drop([&#39;Parch&#39;, &#39;SibSp&#39;, &#39;FamilySize&#39;,&#39;Ticket&#39;,&#39;Cabin&#39;], axis=1) df = [train, test] train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 7.2500 | S | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 71.2833 | C | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 7.9250 | S | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 53.1000 | S | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 8.0500 | S | 1 | 1 | . train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 Fare 0 Embarked 2 Title 0 Alone 0 dtype: int64 . mode = train.Embarked.dropna().mode()[0] for dataset in df: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(mode) train[[&#39;Embarked&#39;, &#39;Survived&#39;]].groupby([&#39;Embarked&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Embarked Survived . 0 C | 0.553571 | . 1 Q | 0.389610 | . 2 S | 0.339009 | . Converting categorical values to numeric values . for dataset in df: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2} ).astype(int) train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 7.2500 | 0 | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 71.2833 | 1 | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 7.9250 | 0 | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 53.1000 | 0 | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 8.0500 | 0 | 1 | 1 | . train.isnull().sum() . Survived 0 Pclass 0 Sex 0 Age 0 Fare 0 Embarked 0 Title 0 Alone 0 dtype: int64 . test.isnull().sum() . PassengerId 0 Pclass 0 Sex 0 Age 0 Fare 1 Embarked 0 Title 0 Alone 0 dtype: int64 . test[&#39;Fare&#39;].fillna(test[&#39;Fare&#39;].dropna().median(), inplace=True) test.head() . PassengerId Pclass Sex Age Fare Embarked Title Alone . 0 892 | 3 | 0 | 2.0 | 7.8292 | 2 | 1 | 1 | . 1 893 | 3 | 1 | 2.0 | 7.0000 | 0 | 3 | 0 | . 2 894 | 2 | 0 | 3.0 | 9.6875 | 2 | 1 | 1 | . 3 895 | 3 | 0 | 1.0 | 8.6625 | 0 | 1 | 1 | . 4 896 | 3 | 1 | 1.0 | 12.2875 | 0 | 3 | 0 | . train[&#39;FareBand&#39;] = pd.qcut(train[&#39;Fare&#39;], 4) train[[&#39;FareBand&#39;, &#39;Survived&#39;]].groupby([&#39;FareBand&#39;], as_index=False).mean().sort_values(by=&#39;FareBand&#39;, ascending=True) . FareBand Survived . 0 (-0.001, 7.91] | 0.197309 | . 1 (7.91, 14.454] | 0.303571 | . 2 (14.454, 31.0] | 0.454955 | . 3 (31.0, 512.329] | 0.581081 | . for dataset in df: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.91, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.91) &amp; (dataset[&#39;Fare&#39;] &lt;= 14.454), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 14.454) &amp; (dataset[&#39;Fare&#39;] &lt;= 31), &#39;Fare&#39;] = 2 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 31, &#39;Fare&#39;] = 3 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].astype(int) train = train.drop([&#39;FareBand&#39;], axis=1) df = [train, test] train.head() . Survived Pclass Sex Age Fare Embarked Title Alone . 0 0 | 3 | 0 | 1.0 | 0 | 0 | 1 | 0 | . 1 1 | 1 | 1 | 2.0 | 3 | 1 | 3 | 0 | . 2 1 | 3 | 1 | 1.0 | 1 | 0 | 2 | 1 | . 3 1 | 1 | 1 | 2.0 | 3 | 0 | 3 | 0 | . 4 0 | 3 | 0 | 2.0 | 1 | 0 | 1 | 1 | . test.head() . PassengerId Pclass Sex Age Fare Embarked Title Alone . 0 892 | 3 | 0 | 2.0 | 0 | 2 | 1 | 1 | . 1 893 | 3 | 1 | 2.0 | 0 | 0 | 3 | 0 | . 2 894 | 2 | 0 | 3.0 | 1 | 2 | 1 | 1 | . 3 895 | 3 | 0 | 1.0 | 1 | 0 | 1 | 1 | . 4 896 | 3 | 1 | 1.0 | 1 | 0 | 3 | 0 | . Modelling . Now we can train a model and predict the required solution.Best Model should be selected from these ML algorithms. . Logistic Regression | KNN or k-Nearest Neighbors | Support Vector Machines | Naive Bayes classifier | Decision Tree | Random Forrest | . X_train = train.drop(&quot;Survived&quot;, axis=1) Y_train = train[&quot;Survived&quot;] X_test = test.drop(&quot;PassengerId&quot;, axis=1).copy() X_train.shape, Y_train.shape, X_test.shape . ((891, 7), (891,), (418, 7)) . logreg = LogisticRegression() logreg.fit(X_train, Y_train) Y_pred = logreg.predict(X_test) acc_log = round(logreg.score(X_train, Y_train) * 100, 2) acc_log . 78.56 . coeff_df = pd.DataFrame(train.columns.delete(0)) coeff_df.columns = [&#39;Feature&#39;] coeff_df[&quot;Correlation&quot;] = pd.Series(logreg.coef_[0]) coeff_df.sort_values(by=&#39;Correlation&#39;, ascending=False) . Feature Correlation . 1 Sex | 2.148684 | . 5 Title | 0.413368 | . 4 Embarked | 0.313126 | . 6 Alone | 0.051407 | . 2 Age | -0.032112 | . 3 Fare | -0.037693 | . 0 Pclass | -0.996006 | . svc = SVC() svc.fit(X_train, Y_train) Y_pred = svc.predict(X_test) acc_svc = round(svc.score(X_train, Y_train) * 100, 2) acc_svc . 78.34 . knn = KNeighborsClassifier(n_neighbors = 3) knn.fit(X_train, Y_train) Y_pred = knn.predict(X_test) acc_knn = round(knn.score(X_train, Y_train) * 100, 2) acc_knn . 83.84 . gaussian = GaussianNB() gaussian.fit(X_train, Y_train) Y_pred = gaussian.predict(X_test) acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2) acc_gaussian . 76.99 . decision_tree = DecisionTreeClassifier() decision_tree.fit(X_train, Y_train) Y_pred = decision_tree.predict(X_test) acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2) acc_decision_tree . 87.21 . random_forest = RandomForestClassifier(n_estimators=100) random_forest.fit(X_train, Y_train) Y_pred = random_forest.predict(X_test) random_forest.score(X_train, Y_train) acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2) acc_random_forest . 87.21 . models = pd.DataFrame({ &#39;Model&#39;: [&#39;Support Vector Machines&#39;, &#39;KNN&#39;, &#39;Logistic Regression&#39;, &#39;Random Forest&#39;,&#39;Naive Bayes&#39;,&#39;Decision Tree&#39;], &#39;Score&#39;: [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_decision_tree]}) models.sort_values(by=&#39;Score&#39;, ascending=False) . Model Score . 3 Random Forest | 87.21 | . 5 Decision Tree | 87.21 | . 1 KNN | 83.84 | . 2 Logistic Regression | 78.56 | . 0 Support Vector Machines | 78.34 | . 4 Naive Bayes | 76.99 | . submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: Y_pred }) . Any suggestions to improve our score are most welcome. . . twitter: https://twitter.com/jithinharidaas/ .",
            "url": "https://jithinharidas.github.io/paranormal-distributions/2020/09/13/Exploring-Titanic-Dataset.html",
            "relUrl": "/2020/09/13/Exploring-Titanic-Dataset.html",
            "date": " • Sep 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploring House Price Dataset",
            "content": ". Introduction . I decided to enter Kaggle&#39;s advanced regression techniques competition and take you along on the ride. Please stay around if you&#39;re new to machine learning and want to witness a project from start to finish. . Objective &amp; Data . The competition goal is to predict sale prices for homes. We’re given a training and testing data set in csv format as well as a data dictionary . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error sns.set(style=&#39;ticks&#39;, color_codes = True) . pd.set_option(&#39;display.max_rows&#39;, 500) pd.set_option(&#39;display.max_columns&#39;, 500) . Importing DataSet . train = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) . Appending train and test for ease of data cleaning . df = train.append(test , sort = True) . some data exploration . print(&#39;Train:&#39;,train.shape) print(&#39;Test:&#39;,test.shape) print(&#39;DataFrame:&#39;,df.shape) . NameError Traceback (most recent call last) &lt;ipython-input-2-24720b2feb94&gt; in &lt;module&gt; -&gt; 1 print(&#39;Train:&#39;,train.shape) 2 print(&#39;Test:&#39;,test.shape) 3 print(&#39;DataFrame:&#39;,df.shape) NameError: name &#39;train&#39; is not defined . df.head() . NameError Traceback (most recent call last) &lt;ipython-input-3-c42a15b2c7cf&gt; in &lt;module&gt; -&gt; 1 df.head() NameError: name &#39;df&#39; is not defined . df.isnull().sum() . 1stFlrSF 0 2ndFlrSF 0 3SsnPorch 0 Alley 2721 BedroomAbvGr 0 BldgType 0 BsmtCond 82 BsmtExposure 82 BsmtFinSF1 1 BsmtFinSF2 1 BsmtFinType1 79 BsmtFinType2 80 BsmtFullBath 2 BsmtHalfBath 2 BsmtQual 81 BsmtUnfSF 1 CentralAir 0 Condition1 0 Condition2 0 Electrical 1 EnclosedPorch 0 ExterCond 0 ExterQual 0 Exterior1st 1 Exterior2nd 1 Fence 2348 FireplaceQu 1420 Fireplaces 0 Foundation 0 FullBath 0 Functional 2 GarageArea 1 GarageCars 1 GarageCond 159 GarageFinish 159 GarageQual 159 GarageType 157 GarageYrBlt 159 GrLivArea 0 HalfBath 0 Heating 0 HeatingQC 0 HouseStyle 0 Id 0 KitchenAbvGr 0 KitchenQual 1 LandContour 0 LandSlope 0 LotArea 0 LotConfig 0 LotFrontage 486 LotShape 0 LowQualFinSF 0 MSSubClass 0 MSZoning 4 MasVnrArea 23 MasVnrType 24 MiscFeature 2814 MiscVal 0 MoSold 0 Neighborhood 0 OpenPorchSF 0 OverallCond 0 OverallQual 0 PavedDrive 0 PoolArea 0 PoolQC 2909 RoofMatl 0 RoofStyle 0 SaleCondition 0 SalePrice 1459 SaleType 1 ScreenPorch 0 Street 0 TotRmsAbvGrd 0 TotalBsmtSF 1 Utilities 2 WoodDeckSF 0 YearBuilt 0 YearRemodAdd 0 YrSold 0 dtype: int64 . there are a lot of missing values, let&#39;s seperate columns into numerical and categorical . df.dtypes . 1stFlrSF int64 2ndFlrSF int64 3SsnPorch int64 Alley object BedroomAbvGr int64 BldgType object BsmtCond object BsmtExposure object BsmtFinSF1 float64 BsmtFinSF2 float64 BsmtFinType1 object BsmtFinType2 object BsmtFullBath float64 BsmtHalfBath float64 BsmtQual object BsmtUnfSF float64 CentralAir object Condition1 object Condition2 object Electrical object EnclosedPorch int64 ExterCond object ExterQual object Exterior1st object Exterior2nd object Fence object FireplaceQu object Fireplaces int64 Foundation object FullBath int64 Functional object GarageArea float64 GarageCars float64 GarageCond object GarageFinish object GarageQual object GarageType object GarageYrBlt float64 GrLivArea int64 HalfBath int64 Heating object HeatingQC object HouseStyle object Id int64 KitchenAbvGr int64 KitchenQual object LandContour object LandSlope object LotArea int64 LotConfig object LotFrontage float64 LotShape object LowQualFinSF int64 MSSubClass int64 MSZoning object MasVnrArea float64 MasVnrType object MiscFeature object MiscVal int64 MoSold int64 Neighborhood object OpenPorchSF int64 OverallCond int64 OverallQual int64 PavedDrive object PoolArea int64 PoolQC object RoofMatl object RoofStyle object SaleCondition object SalePrice float64 SaleType object ScreenPorch int64 Street object TotRmsAbvGrd int64 TotalBsmtSF float64 Utilities object WoodDeckSF int64 YearBuilt int64 YearRemodAdd int64 YrSold int64 dtype: object . Data Cleaning . cat = list(df.select_dtypes(&#39;object&#39;)) num = list(df.select_dtypes([&#39;int64&#39;,&#39;float64&#39;])) . cat . [&#39;Alley&#39;, &#39;BldgType&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtQual&#39;, &#39;CentralAir&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;Electrical&#39;, &#39;ExterCond&#39;, &#39;ExterQual&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;Fence&#39;, &#39;FireplaceQu&#39;, &#39;Foundation&#39;, &#39;Functional&#39;, &#39;GarageCond&#39;, &#39;GarageFinish&#39;, &#39;GarageQual&#39;, &#39;GarageType&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;HouseStyle&#39;, &#39;KitchenQual&#39;, &#39;LandContour&#39;, &#39;LandSlope&#39;, &#39;LotConfig&#39;, &#39;LotShape&#39;, &#39;MSZoning&#39;, &#39;MasVnrType&#39;, &#39;MiscFeature&#39;, &#39;Neighborhood&#39;, &#39;PavedDrive&#39;, &#39;PoolQC&#39;, &#39;RoofMatl&#39;, &#39;RoofStyle&#39;, &#39;SaleCondition&#39;, &#39;SaleType&#39;, &#39;Street&#39;, &#39;Utilities&#39;] . num . [&#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;3SsnPorch&#39;, &#39;BedroomAbvGr&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;BsmtUnfSF&#39;, &#39;EnclosedPorch&#39;, &#39;Fireplaces&#39;, &#39;FullBath&#39;, &#39;GarageArea&#39;, &#39;GarageCars&#39;, &#39;GarageYrBlt&#39;, &#39;GrLivArea&#39;, &#39;HalfBath&#39;, &#39;Id&#39;, &#39;KitchenAbvGr&#39;, &#39;LotArea&#39;, &#39;LotFrontage&#39;, &#39;LowQualFinSF&#39;, &#39;MSSubClass&#39;, &#39;MasVnrArea&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;OpenPorchSF&#39;, &#39;OverallCond&#39;, &#39;OverallQual&#39;, &#39;PoolArea&#39;, &#39;SalePrice&#39;, &#39;ScreenPorch&#39;, &#39;TotRmsAbvGrd&#39;, &#39;TotalBsmtSF&#39;, &#39;WoodDeckSF&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;YrSold&#39;] . na = df[num].isnull().sum() na = na[na &gt; 0] na = na.sort_values(ascending=False) print(na) . SalePrice 1459 LotFrontage 486 GarageYrBlt 159 MasVnrArea 23 BsmtHalfBath 2 BsmtFullBath 2 TotalBsmtSF 1 GarageCars 1 GarageArea 1 BsmtUnfSF 1 BsmtFinSF2 1 BsmtFinSF1 1 dtype: int64 . na = df[cat].isnull().sum() na = na[na &gt; 0] na = na.sort_values(ascending=False) print(na) . PoolQC 2909 MiscFeature 2814 Alley 2721 Fence 2348 FireplaceQu 1420 GarageQual 159 GarageFinish 159 GarageCond 159 GarageType 157 BsmtCond 82 BsmtExposure 82 BsmtQual 81 BsmtFinType2 80 BsmtFinType1 79 MasVnrType 24 MSZoning 4 Utilities 2 Functional 2 Electrical 1 Exterior1st 1 Exterior2nd 1 SaleType 1 KitchenQual 1 dtype: int64 . Numerical columns missing values . df.LotFrontage.fillna(df.LotFrontage.median(), inplace=True) df.GarageYrBlt.fillna(0, inplace=True) . in step above we replaced missing values in LotFrontage with median so that donot change shape of data . in GarageYrBlt null value means there is no garage so we replaced it with 0. . in all other numerical columns null value indicate not available so we replace it with 0 below. . df.MasVnrArea.fillna(0, inplace=True) df.BsmtHalfBath.fillna(0, inplace=True) df.BsmtFullBath.fillna(0, inplace=True) df.GarageArea.fillna(0, inplace=True) df.GarageCars.fillna(0, inplace=True) df.TotalBsmtSF.fillna(0, inplace=True) df.BsmtUnfSF.fillna(0, inplace=True) df.BsmtFinSF2.fillna(0, inplace=True) df.BsmtFinSF1.fillna(0, inplace=True) . df[num].isnull().sum() . 1stFlrSF 0 2ndFlrSF 0 3SsnPorch 0 BedroomAbvGr 0 BsmtFinSF1 0 BsmtFinSF2 0 BsmtFullBath 0 BsmtHalfBath 0 BsmtUnfSF 0 EnclosedPorch 0 Fireplaces 0 FullBath 0 GarageArea 0 GarageCars 0 GarageYrBlt 0 GrLivArea 0 HalfBath 0 Id 0 KitchenAbvGr 0 LotArea 0 LotFrontage 0 LowQualFinSF 0 MSSubClass 0 MasVnrArea 0 MiscVal 0 MoSold 0 OpenPorchSF 0 OverallCond 0 OverallQual 0 PoolArea 0 SalePrice 1459 ScreenPorch 0 TotRmsAbvGrd 0 TotalBsmtSF 0 WoodDeckSF 0 YearBuilt 0 YearRemodAdd 0 YrSold 0 dtype: int64 . so all missing values treated exept SalePrice, Here SalePrice is that of test data which we have to predict. . Categorical columns missing values . df.PoolQC.fillna(&#39;NA&#39;, inplace=True) df.MiscFeature.fillna(&#39;NA&#39;, inplace=True) df.Alley.fillna(&#39;NA&#39;, inplace=True) df.Fence.fillna(&#39;NA&#39;, inplace=True) df.FireplaceQu.fillna(&#39;NA&#39;, inplace=True) df.GarageCond.fillna(&#39;NA&#39;, inplace=True) df.GarageQual.fillna(&#39;NA&#39;, inplace=True) df.GarageFinish.fillna(&#39;NA&#39;, inplace=True) df.GarageType.fillna(&#39;NA&#39;, inplace=True) df.BsmtExposure.fillna(&#39;NA&#39;, inplace=True) df.BsmtCond.fillna(&#39;NA&#39;, inplace=True) df.BsmtQual.fillna(&#39;NA&#39;, inplace=True) df.BsmtFinType2.fillna(&#39;NA&#39;, inplace=True) df.BsmtFinType1.fillna(&#39;NA&#39;, inplace=True) df.MasVnrType.fillna(&#39;None&#39;, inplace=True) df.Exterior2nd.fillna(&#39;None&#39;, inplace=True) . all these columns similar to numerical column where null value meant not available we replace it with NA or None according to the data dictionary and rest we fill the mode. . df.Functional.fillna(df.Functional.mode()[0], inplace=True) df.Utilities.fillna(df.Utilities.mode()[0], inplace=True) df.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True) df.SaleType.fillna(df.SaleType.mode()[0], inplace=True) df.KitchenQual.fillna(df.KitchenQual.mode()[0], inplace=True) df.Electrical.fillna(df.Electrical.mode()[0], inplace=True) df.MSZoning.fillna(df.MSZoning.mode()[0], inplace=True) . df[cat].isnull().sum() . Alley 0 BldgType 0 BsmtCond 0 BsmtExposure 0 BsmtFinType1 0 BsmtFinType2 0 BsmtQual 0 CentralAir 0 Condition1 0 Condition2 0 Electrical 0 ExterCond 0 ExterQual 0 Exterior1st 0 Exterior2nd 0 Fence 0 FireplaceQu 0 Foundation 0 Functional 0 GarageCond 0 GarageFinish 0 GarageQual 0 GarageType 0 Heating 0 HeatingQC 0 HouseStyle 0 KitchenQual 0 LandContour 0 LandSlope 0 LotConfig 0 LotShape 0 MSZoning 0 MasVnrType 0 MiscFeature 0 Neighborhood 0 PavedDrive 0 PoolQC 0 RoofMatl 0 RoofStyle 0 SaleCondition 0 SaleType 0 Street 0 Utilities 0 dtype: int64 . all null values treated with no missing variable currently. . Exploratory Data Analysis (EDA) . This is often where we begin our data visualisation adventure. In machine learning, EDA is used to explore the importance of our data. . def boxplot(var): sns.catplot(x=var, y=&#39;SalePrice&#39;,data = train, kind=&#39;box&#39;) . boxplot(&#39;Alley&#39;) boxplot(&#39;BldgType&#39;) boxplot(&#39;BsmtCond&#39;) boxplot(&#39;BsmtFinType1&#39;) . Alley has a impact on price as we can see from boxplot. BldgType,BsmtCond,BsmtFinType1 has a very small impact. . boxplot(&#39;BsmtExposure&#39;) boxplot(&#39;BsmtFinType2&#39;) boxplot(&#39;BsmtQual&#39;) boxplot(&#39;CentralAir&#39;) . CentralAir,BsmtQual has a impact on price as we can see from boxplot. BsmtFinType2,BsmtExposure has a very small impact. . boxplot(&#39;Condition1&#39;) boxplot(&#39;Condition2&#39;) boxplot(&#39;Electrical&#39;) boxplot(&#39;ExterCond&#39;) . ExterCond,Condition2,Condition1 has a impact on price as we can see from boxplot. Electrical has a very small impact. . boxplot(&#39;ExterQual&#39;) boxplot(&#39;Exterior1st&#39;) boxplot(&#39;Exterior2nd&#39;) boxplot(&#39;Fence&#39;) . ExterQual has a impact on price as we can see from boxplot. Fence,Exterior1st,Exterior2nd has a very small impact. . boxplot(&#39;FireplaceQu&#39;) boxplot(&#39;Foundation&#39;) boxplot(&#39;Functional&#39;) boxplot(&#39;GarageCond&#39;) . FireplaceQu,Foundation,Functional,GarageCond has a very small impact. also excellent fireplacequal shows a jump in price. . boxplot(&#39;GarageFinish&#39;) boxplot(&#39;GarageQual&#39;) boxplot(&#39;GarageType&#39;) boxplot(&#39;Heating&#39;) . GarageQual,GarageFinish has a impact on price as we can see from boxplot. Heating,GarageType has a very small impact. . boxplot(&#39;HeatingQC&#39;) boxplot(&#39;HouseStyle&#39;) boxplot(&#39;KitchenQual&#39;) boxplot(&#39;LandContour&#39;) . HouseStyle,KitchenQual has a impact on price as we can see from boxplot. LandContour,HeatingQC has a very small impact. . boxplot(&#39;LandSlope&#39;) boxplot(&#39;LotConfig&#39;) boxplot(&#39;LotShape&#39;) boxplot(&#39;MSZoning&#39;) . MSZoning has a impact on price as we can see from boxplot. LotShape,LotConfig,LandSlope has no impact. . boxplot(&#39;MasVnrType&#39;) boxplot(&#39;MiscFeature&#39;) boxplot(&#39;Neighborhood&#39;) boxplot(&#39;PavedDrive&#39;) . PavedDrive,Neighborhood,MiscFeature has a impact on price. MasVnrType has a very small impact,changes drastically with stone. . boxplot(&#39;PoolQC&#39;) boxplot(&#39;RoofMatl&#39;) boxplot(&#39;RoofStyle&#39;) boxplot(&#39;SaleCondition&#39;) . RoofStyle,RoofMatl,PoolQC has a impact on price as we can see from boxplot. SaleCondition has a very small impact and increase with Partial. . boxplot(&#39;SaleType&#39;) boxplot(&#39;Street&#39;) boxplot(&#39;Utilities&#39;) . SaleType has a impact on price as we can see from boxplot. Street,Utilities has a very small impact. . High Dependency: . CentralAir BsmtQual Alley ExterCond Condition2 Condition1 ExterQual GarageQual GarageFinish HouseStyle KitchenQual MSZoning PavedDrive Neighborhood MiscFeature RoofStyle RoofMatl PoolQC SaleType . Modarate Dependency: . BldgType BsmtCond BsmtFinType1 BsmtFinType2 BsmtExposure Electrical Fence Exterior1st Exterior2nd FireplaceQu Foundation Functional GarageCond Heating GarageType LandContour HeatingQC SaleCondition Street Utilities . Low Dependency: . LotShape LotConfig LandSlope MasVnrType . corrmat = df.corr() f, ax = plt.subplots(figsize=(24, 9)) sns.heatmap(corrmat, vmax=.8, square=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c89ad25c08&gt; . k = 20 cols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index cm = np.corrcoef(df[cols].values.T) f, ax = plt.subplots(figsize=(24, 9)) sns.set(font_scale=1.25) hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.1f&#39;, annot_kws={&#39;size&#39;: 10}, yticklabels=cols.values, xticklabels=cols.values) plt.show() . High Correlated . OverallQual GrLivArea GarageCars TotalBsmtSF 1stFlrSF FullBath TotRmsAbvGrd YearBuilt YrRemodAdd MasVnrArea Fireplaces BsmtFinSF1 LotFrontage WoodDeckSF 2ndFlrSF OpenPorchSF HalfBath LotArea . It&#39;s needed to create dummy vars and map categorical features in order to run ML model. . Create mapping for categorical features that can be ranked. . df.Alley = df.Alley.map({&#39;NA&#39;:0, &#39;Grvl&#39;:1, &#39;Pave&#39;:2}) df.BsmtCond = df.BsmtCond.map({&#39;NA&#39;:0, &#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.BsmtExposure = df.BsmtExposure.map({&#39;NA&#39;:0, &#39;No&#39;:1, &#39;Mn&#39;:2, &#39;Av&#39;:3, &#39;Gd&#39;:4}) df[&#39;BsmtFinType1&#39;] = df[&#39;BsmtFinType1&#39;].map({&#39;NA&#39;:0, &#39;Unf&#39;:1, &#39;LwQ&#39;:2, &#39;Rec&#39;:3, &#39;BLQ&#39;:4, &#39;ALQ&#39;:5, &#39;GLQ&#39;:6}) df[&#39;BsmtFinType2&#39;] = df[&#39;BsmtFinType2&#39;].map({&#39;NA&#39;:0, &#39;Unf&#39;:1, &#39;LwQ&#39;:2, &#39;Rec&#39;:3, &#39;BLQ&#39;:4, &#39;ALQ&#39;:5, &#39;GLQ&#39;:6}) df.BsmtQual = df.BsmtQual.map({&#39;NA&#39;:0, &#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.ExterCond = df.ExterCond.map({&#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.ExterQual = df.ExterQual.map({&#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.FireplaceQu = df.FireplaceQu.map({&#39;NA&#39;:0, &#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.Functional = df.Functional.map({&#39;Sal&#39;:1, &#39;Sev&#39;:2, &#39;Maj2&#39;:3, &#39;Maj1&#39;:4, &#39;Mod&#39;:5, &#39;Min2&#39;:6, &#39;Min1&#39;:7, &#39;Typ&#39;:8}) df.GarageCond = df.GarageCond.map({&#39;NA&#39;:0, &#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.GarageQual = df.GarageQual.map({&#39;NA&#39;:0, &#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.HeatingQC = df.HeatingQC.map({&#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.KitchenQual = df.KitchenQual.map({&#39;Po&#39;:1, &#39;Fa&#39;:2, &#39;TA&#39;:3, &#39;Gd&#39;:4, &#39;Ex&#39;:5}) df.LandSlope = df.LandSlope.map({&#39;Sev&#39;:1, &#39;Mod&#39;:2, &#39;Gtl&#39;:3}) df.PavedDrive = df.PavedDrive.map({&#39;N&#39;:1, &#39;P&#39;:2, &#39;Y&#39;:3}) df.PoolQC = df.PoolQC.map({&#39;NA&#39;:0, &#39;Fa&#39;:1, &#39;TA&#39;:2, &#39;Gd&#39;:3, &#39;Ex&#39;:4}) df.Street = df.Street.map({&#39;Grvl&#39;:1, &#39;Pave&#39;:2}) df.Utilities = df.Utilities.map({&#39;ELO&#39;:1, &#39;NoSeWa&#39;:2, &#39;NoSewr&#39;:3, &#39;AllPub&#39;:4}) . new_num = [&#39;Alley&#39;,&#39;BsmtCond&#39;,&#39;BsmtExposure&#39;,&#39;BsmtFinType1&#39;,&#39;BsmtFinType2&#39;,&#39;BsmtQual&#39;, &#39;ExterCond&#39;,&#39;ExterQual&#39;,&#39;FireplaceQu&#39;,&#39;Functional&#39;,&#39;GarageCond&#39;, &#39;GarageQual&#39;,&#39;HeatingQC&#39;,&#39;KitchenQual&#39;,&#39;LandSlope&#39;,&#39;PavedDrive&#39;,&#39;PoolQC&#39;, &#39;Street&#39;,&#39;Utilities&#39;] num = num + new_num for i in new_num: cat.remove(i) . all these are now numerical so added to num list and removed from cat list. . num . [&#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;3SsnPorch&#39;, &#39;BedroomAbvGr&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;BsmtUnfSF&#39;, &#39;EnclosedPorch&#39;, &#39;Fireplaces&#39;, &#39;FullBath&#39;, &#39;GarageArea&#39;, &#39;GarageCars&#39;, &#39;GarageYrBlt&#39;, &#39;GrLivArea&#39;, &#39;HalfBath&#39;, &#39;Id&#39;, &#39;KitchenAbvGr&#39;, &#39;LotArea&#39;, &#39;LotFrontage&#39;, &#39;LowQualFinSF&#39;, &#39;MSSubClass&#39;, &#39;MasVnrArea&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;OpenPorchSF&#39;, &#39;OverallCond&#39;, &#39;OverallQual&#39;, &#39;PoolArea&#39;, &#39;SalePrice&#39;, &#39;ScreenPorch&#39;, &#39;TotRmsAbvGrd&#39;, &#39;TotalBsmtSF&#39;, &#39;WoodDeckSF&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;YrSold&#39;, &#39;Alley&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;, &#39;BsmtQual&#39;, &#39;ExterCond&#39;, &#39;ExterQual&#39;, &#39;FireplaceQu&#39;, &#39;Functional&#39;, &#39;GarageCond&#39;, &#39;GarageQual&#39;, &#39;HeatingQC&#39;, &#39;KitchenQual&#39;, &#39;LandSlope&#39;, &#39;PavedDrive&#39;, &#39;PoolQC&#39;, &#39;Street&#39;, &#39;Utilities&#39;] . cat . [&#39;BldgType&#39;, &#39;CentralAir&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;Electrical&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;Fence&#39;, &#39;Foundation&#39;, &#39;GarageFinish&#39;, &#39;GarageType&#39;, &#39;Heating&#39;, &#39;HouseStyle&#39;, &#39;LandContour&#39;, &#39;LotConfig&#39;, &#39;LotShape&#39;, &#39;MSZoning&#39;, &#39;MasVnrType&#39;, &#39;MiscFeature&#39;, &#39;Neighborhood&#39;, &#39;RoofMatl&#39;, &#39;RoofStyle&#39;, &#39;SaleCondition&#39;, &#39;SaleType&#39;] . mssubclass is a categorical data not numerical therefore we can map it to category. . df.MSSubClass = df.MSSubClass.map({20:&#39;class1&#39;, 30:&#39;class2&#39;, 40:&#39;class3&#39;, 45:&#39;class4&#39;, 50:&#39;class5&#39;, 60:&#39;class6&#39;, 70:&#39;class7&#39;, 75:&#39;class8&#39;, 80:&#39;class9&#39;, 85:&#39;class10&#39;, 90:&#39;class11&#39;, 120:&#39;class12&#39;, 150:&#39;class13&#39;, 160:&#39;class14&#39;, 180:&#39;class15&#39;, 190:&#39;class16&#39;}) . num.remove(&#39;MSSubClass&#39;) cat.append(&#39;MSSubClass&#39;) . There are 4 year specifing columns.It would be more helpful if it showed how long back building built or just age of building. . df[&#39;Age&#39;] = df.YrSold - df.YearBuilt df[&#39;AgeRemod&#39;] = df.YrSold - df.YearRemodAdd df[&#39;AgeGarage&#39;] = df.YrSold - df.GarageYrBlt . there can be -ve values if no garage build. . max_AgeGarage = np.max(df.AgeGarage[df.AgeGarage &lt; 1000]) df[&#39;AgeGarage&#39;] = df[&#39;AgeGarage&#39;].map(lambda x: max_AgeGarage if x &gt; 1000 else x) df.Age = df.Age.map(lambda x: 0 if x &lt; 0 else x) df.AgeRemod = df.AgeRemod.map(lambda x: 0 if x &lt; 0 else x) df.AgeGarage = df.AgeGarage.map(lambda x: 0 if x &lt; 0 else x) . df=df.drop([&#39;YrSold&#39;,&#39;YearBuilt&#39;,&#39;YearRemodAdd&#39;,&#39;GarageYrBlt&#39;],axis=1) for i in [&#39;YrSold&#39;,&#39;YearBuilt&#39;,&#39;YearRemodAdd&#39;,&#39;GarageYrBlt&#39;]: num.remove(i) num = num + [&#39;Age&#39;,&#39;AgeRemod&#39;,&#39;AgeGarage&#39;] . Create dummy for categorical features that cannot be ranked. . dummy_drop = [] for i in cat: dummy_drop += [ i+&#39;_&#39;+str(df[i].unique()[-1]) ] df = pd.get_dummies(df,columns=cat) df = df.drop(dummy_drop,axis=1) . df.head() . 1stFlrSF 2ndFlrSF 3SsnPorch Alley BedroomAbvGr BsmtCond BsmtExposure BsmtFinSF1 BsmtFinSF2 BsmtFinType1 BsmtFinType2 BsmtFullBath BsmtHalfBath BsmtQual BsmtUnfSF EnclosedPorch ExterCond ExterQual FireplaceQu Fireplaces FullBath Functional GarageArea GarageCars GarageCond GarageQual GrLivArea HalfBath HeatingQC Id KitchenAbvGr KitchenQual LandSlope LotArea LotFrontage LowQualFinSF MasVnrArea MiscVal MoSold OpenPorchSF OverallCond OverallQual PavedDrive PoolArea PoolQC SalePrice ScreenPorch Street TotRmsAbvGrd TotalBsmtSF Utilities WoodDeckSF Age AgeRemod AgeGarage BldgType_1Fam BldgType_2fmCon BldgType_Duplex BldgType_TwnhsE CentralAir_Y Condition1_Artery Condition1_Feedr Condition1_Norm Condition1_PosA Condition1_PosN Condition1_RRAe Condition1_RRAn Condition1_RRNn Condition2_Artery Condition2_Feedr Condition2_Norm Condition2_PosA Condition2_PosN Condition2_RRAn Condition2_RRNn Electrical_FuseA Electrical_FuseF Electrical_FuseP Electrical_SBrkr Exterior1st_AsbShng Exterior1st_AsphShn Exterior1st_BrkComm Exterior1st_BrkFace Exterior1st_CemntBd Exterior1st_HdBoard Exterior1st_ImStucc Exterior1st_MetalSd Exterior1st_Plywood Exterior1st_Stone Exterior1st_Stucco Exterior1st_VinylSd Exterior1st_Wd Sdng Exterior1st_WdShing Exterior2nd_AsbShng Exterior2nd_AsphShn Exterior2nd_Brk Cmn Exterior2nd_BrkFace Exterior2nd_CBlock Exterior2nd_CmentBd Exterior2nd_HdBoard Exterior2nd_ImStucc Exterior2nd_MetalSd Exterior2nd_Other Exterior2nd_Plywood Exterior2nd_Stone Exterior2nd_Stucco Exterior2nd_VinylSd Exterior2nd_Wd Sdng Exterior2nd_Wd Shng Fence_GdPrv Fence_GdWo Fence_MnPrv Fence_NA Foundation_BrkTil Foundation_CBlock Foundation_PConc Foundation_Slab Foundation_Wood GarageFinish_Fin GarageFinish_RFn GarageFinish_Unf GarageType_Attchd GarageType_Basment GarageType_BuiltIn GarageType_CarPort GarageType_Detchd GarageType_NA Heating_GasA Heating_GasW Heating_Grav Heating_OthW Heating_Wall HouseStyle_1.5Fin HouseStyle_1.5Unf HouseStyle_1Story HouseStyle_2.5Unf HouseStyle_2Story HouseStyle_SFoyer HouseStyle_SLvl LandContour_Bnk LandContour_Low LandContour_Lvl LotConfig_Corner LotConfig_CulDSac LotConfig_FR2 LotConfig_Inside LotShape_IR1 LotShape_IR2 LotShape_Reg MSZoning_C (all) MSZoning_FV MSZoning_RL MSZoning_RM MasVnrType_BrkFace MasVnrType_None MasVnrType_Stone MiscFeature_Gar2 MiscFeature_NA MiscFeature_Othr MiscFeature_Shed Neighborhood_Blmngtn Neighborhood_BrDale Neighborhood_BrkSide Neighborhood_ClearCr Neighborhood_CollgCr Neighborhood_Crawfor Neighborhood_Edwards Neighborhood_Gilbert Neighborhood_IDOTRR Neighborhood_MeadowV Neighborhood_Mitchel Neighborhood_NAmes Neighborhood_NPkVill Neighborhood_NWAmes Neighborhood_NoRidge Neighborhood_NridgHt Neighborhood_OldTown Neighborhood_SWISU Neighborhood_Sawyer Neighborhood_SawyerW Neighborhood_Somerst Neighborhood_StoneBr Neighborhood_Timber Neighborhood_Veenker RoofMatl_CompShg RoofMatl_Membran RoofMatl_Metal RoofMatl_Roll RoofMatl_Tar&amp;Grv RoofMatl_WdShake RoofMatl_WdShngl RoofStyle_Flat RoofStyle_Gable RoofStyle_Gambrel RoofStyle_Hip RoofStyle_Mansard SaleCondition_Abnorml SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Normal SaleCondition_Partial SaleType_COD SaleType_CWD SaleType_Con SaleType_ConLD SaleType_ConLI SaleType_ConLw SaleType_New SaleType_WD MSSubClass_class1 MSSubClass_class10 MSSubClass_class11 MSSubClass_class12 MSSubClass_class14 MSSubClass_class15 MSSubClass_class16 MSSubClass_class2 MSSubClass_class3 MSSubClass_class4 MSSubClass_class5 MSSubClass_class6 MSSubClass_class7 MSSubClass_class8 MSSubClass_class9 . 0 | 856 | 854 | 0 | 0 | 3 | 3 | 1 | 706.0 | 0.0 | 6 | 1 | 1.0 | 0.0 | 4 | 150.0 | 0 | 3 | 4 | 0 | 0 | 2 | 8 | 548.0 | 2.0 | 3 | 3 | 1710 | 1 | 5 | 1 | 1 | 4 | 3 | 8450 | 65.0 | 0 | 196.0 | 0 | 2 | 61 | 5 | 7 | 3 | 0 | 0 | 208500.0 | 0 | 2 | 8 | 856.0 | 4 | 0 | 5 | 5 | 5.0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1 | 1262 | 0 | 0 | 0 | 3 | 3 | 4 | 978.0 | 0.0 | 5 | 1 | 0.0 | 1.0 | 4 | 284.0 | 0 | 3 | 3 | 3 | 1 | 2 | 8 | 460.0 | 2.0 | 3 | 3 | 1262 | 0 | 5 | 2 | 1 | 3 | 3 | 9600 | 80.0 | 0 | 0.0 | 0 | 5 | 0 | 8 | 6 | 3 | 0 | 0 | 181500.0 | 0 | 2 | 6 | 1262.0 | 4 | 298 | 31 | 31 | 31.0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 | 920 | 866 | 0 | 0 | 3 | 3 | 2 | 486.0 | 0.0 | 6 | 1 | 1.0 | 0.0 | 4 | 434.0 | 0 | 3 | 4 | 3 | 1 | 2 | 8 | 608.0 | 2.0 | 3 | 3 | 1786 | 1 | 5 | 3 | 1 | 4 | 3 | 11250 | 68.0 | 0 | 162.0 | 0 | 9 | 42 | 5 | 7 | 3 | 0 | 0 | 223500.0 | 0 | 2 | 6 | 920.0 | 4 | 0 | 7 | 6 | 7.0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 | 961 | 756 | 0 | 0 | 3 | 4 | 1 | 216.0 | 0.0 | 5 | 1 | 1.0 | 0.0 | 3 | 540.0 | 272 | 3 | 3 | 4 | 1 | 1 | 8 | 642.0 | 3.0 | 3 | 3 | 1717 | 0 | 4 | 4 | 1 | 4 | 3 | 9550 | 60.0 | 0 | 0.0 | 0 | 2 | 35 | 5 | 7 | 3 | 0 | 0 | 140000.0 | 0 | 2 | 7 | 756.0 | 4 | 0 | 91 | 36 | 8.0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 4 | 1145 | 1053 | 0 | 0 | 4 | 3 | 3 | 655.0 | 0.0 | 6 | 1 | 1.0 | 0.0 | 4 | 490.0 | 0 | 3 | 4 | 3 | 1 | 2 | 8 | 836.0 | 3.0 | 3 | 3 | 2198 | 1 | 5 | 5 | 1 | 4 | 3 | 14260 | 84.0 | 0 | 350.0 | 0 | 12 | 84 | 5 | 8 | 3 | 0 | 0 | 250000.0 | 0 | 2 | 9 | 1145.0 | 4 | 192 | 8 | 8 | 8.0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . df.dtypes . 1stFlrSF int64 2ndFlrSF int64 3SsnPorch int64 Alley int64 BedroomAbvGr int64 BsmtCond int64 BsmtExposure int64 BsmtFinSF1 float64 BsmtFinSF2 float64 BsmtFinType1 int64 BsmtFinType2 int64 BsmtFullBath float64 BsmtHalfBath float64 BsmtQual int64 BsmtUnfSF float64 EnclosedPorch int64 ExterCond int64 ExterQual int64 FireplaceQu int64 Fireplaces int64 FullBath int64 Functional int64 GarageArea float64 GarageCars float64 GarageCond int64 GarageQual int64 GrLivArea int64 HalfBath int64 HeatingQC int64 Id int64 KitchenAbvGr int64 KitchenQual int64 LandSlope int64 LotArea int64 LotFrontage float64 LowQualFinSF int64 MasVnrArea float64 MiscVal int64 MoSold int64 OpenPorchSF int64 OverallCond int64 OverallQual int64 PavedDrive int64 PoolArea int64 PoolQC int64 SalePrice float64 ScreenPorch int64 Street int64 TotRmsAbvGrd int64 TotalBsmtSF float64 Utilities int64 WoodDeckSF int64 Age int64 AgeRemod int64 AgeGarage float64 BldgType_1Fam uint8 BldgType_2fmCon uint8 BldgType_Duplex uint8 BldgType_TwnhsE uint8 CentralAir_Y uint8 Condition1_Artery uint8 Condition1_Feedr uint8 Condition1_Norm uint8 Condition1_PosA uint8 Condition1_PosN uint8 Condition1_RRAe uint8 Condition1_RRAn uint8 Condition1_RRNn uint8 Condition2_Artery uint8 Condition2_Feedr uint8 Condition2_Norm uint8 Condition2_PosA uint8 Condition2_PosN uint8 Condition2_RRAn uint8 Condition2_RRNn uint8 Electrical_FuseA uint8 Electrical_FuseF uint8 Electrical_FuseP uint8 Electrical_SBrkr uint8 Exterior1st_AsbShng uint8 Exterior1st_AsphShn uint8 Exterior1st_BrkComm uint8 Exterior1st_BrkFace uint8 Exterior1st_CemntBd uint8 Exterior1st_HdBoard uint8 Exterior1st_ImStucc uint8 Exterior1st_MetalSd uint8 Exterior1st_Plywood uint8 Exterior1st_Stone uint8 Exterior1st_Stucco uint8 Exterior1st_VinylSd uint8 Exterior1st_Wd Sdng uint8 Exterior1st_WdShing uint8 Exterior2nd_AsbShng uint8 Exterior2nd_AsphShn uint8 Exterior2nd_Brk Cmn uint8 Exterior2nd_BrkFace uint8 Exterior2nd_CBlock uint8 Exterior2nd_CmentBd uint8 Exterior2nd_HdBoard uint8 Exterior2nd_ImStucc uint8 Exterior2nd_MetalSd uint8 Exterior2nd_Other uint8 Exterior2nd_Plywood uint8 Exterior2nd_Stone uint8 Exterior2nd_Stucco uint8 Exterior2nd_VinylSd uint8 Exterior2nd_Wd Sdng uint8 Exterior2nd_Wd Shng uint8 Fence_GdPrv uint8 Fence_GdWo uint8 Fence_MnPrv uint8 Fence_NA uint8 Foundation_BrkTil uint8 Foundation_CBlock uint8 Foundation_PConc uint8 Foundation_Slab uint8 Foundation_Wood uint8 GarageFinish_Fin uint8 GarageFinish_RFn uint8 GarageFinish_Unf uint8 GarageType_Attchd uint8 GarageType_Basment uint8 GarageType_BuiltIn uint8 GarageType_CarPort uint8 GarageType_Detchd uint8 GarageType_NA uint8 Heating_GasA uint8 Heating_GasW uint8 Heating_Grav uint8 Heating_OthW uint8 Heating_Wall uint8 HouseStyle_1.5Fin uint8 HouseStyle_1.5Unf uint8 HouseStyle_1Story uint8 HouseStyle_2.5Unf uint8 HouseStyle_2Story uint8 HouseStyle_SFoyer uint8 HouseStyle_SLvl uint8 LandContour_Bnk uint8 LandContour_Low uint8 LandContour_Lvl uint8 LotConfig_Corner uint8 LotConfig_CulDSac uint8 LotConfig_FR2 uint8 LotConfig_Inside uint8 LotShape_IR1 uint8 LotShape_IR2 uint8 LotShape_Reg uint8 MSZoning_C (all) uint8 MSZoning_FV uint8 MSZoning_RL uint8 MSZoning_RM uint8 MasVnrType_BrkFace uint8 MasVnrType_None uint8 MasVnrType_Stone uint8 MiscFeature_Gar2 uint8 MiscFeature_NA uint8 MiscFeature_Othr uint8 MiscFeature_Shed uint8 Neighborhood_Blmngtn uint8 Neighborhood_BrDale uint8 Neighborhood_BrkSide uint8 Neighborhood_ClearCr uint8 Neighborhood_CollgCr uint8 Neighborhood_Crawfor uint8 Neighborhood_Edwards uint8 Neighborhood_Gilbert uint8 Neighborhood_IDOTRR uint8 Neighborhood_MeadowV uint8 Neighborhood_Mitchel uint8 Neighborhood_NAmes uint8 Neighborhood_NPkVill uint8 Neighborhood_NWAmes uint8 Neighborhood_NoRidge uint8 Neighborhood_NridgHt uint8 Neighborhood_OldTown uint8 Neighborhood_SWISU uint8 Neighborhood_Sawyer uint8 Neighborhood_SawyerW uint8 Neighborhood_Somerst uint8 Neighborhood_StoneBr uint8 Neighborhood_Timber uint8 Neighborhood_Veenker uint8 RoofMatl_CompShg uint8 RoofMatl_Membran uint8 RoofMatl_Metal uint8 RoofMatl_Roll uint8 RoofMatl_Tar&amp;Grv uint8 RoofMatl_WdShake uint8 RoofMatl_WdShngl uint8 RoofStyle_Flat uint8 RoofStyle_Gable uint8 RoofStyle_Gambrel uint8 RoofStyle_Hip uint8 RoofStyle_Mansard uint8 SaleCondition_Abnorml uint8 SaleCondition_AdjLand uint8 SaleCondition_Alloca uint8 SaleCondition_Normal uint8 SaleCondition_Partial uint8 SaleType_COD uint8 SaleType_CWD uint8 SaleType_Con uint8 SaleType_ConLD uint8 SaleType_ConLI uint8 SaleType_ConLw uint8 SaleType_New uint8 SaleType_WD uint8 MSSubClass_class1 uint8 MSSubClass_class10 uint8 MSSubClass_class11 uint8 MSSubClass_class12 uint8 MSSubClass_class14 uint8 MSSubClass_class15 uint8 MSSubClass_class16 uint8 MSSubClass_class2 uint8 MSSubClass_class3 uint8 MSSubClass_class4 uint8 MSSubClass_class5 uint8 MSSubClass_class6 uint8 MSSubClass_class7 uint8 MSSubClass_class8 uint8 MSSubClass_class9 uint8 dtype: object . df.shape . (2919, 224) . Split dataset train as first 1460 rows and test as last 1459 rows because we appended a train and test earilier. . X_train = df[:-1459].drop([&#39;SalePrice&#39;,&#39;Id&#39;], axis=1) y_train = df[:-1459][&#39;SalePrice&#39;] X_test = df[-1459:].drop([&#39;SalePrice&#39;,&#39;Id&#39;], axis=1) . from xgboost import XGBRegressor . xgb = XGBRegressor() xgb.fit(X_train, y_train) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=100, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . xgb.score(X_train, y_train) . we have to take only few features as to improve model. . imp = pd.DataFrame(xgb.feature_importances_ ,columns = [&#39;Importance&#39;],index = X_train.columns) imp = imp.sort_values([&#39;Importance&#39;], ascending = False) . imp . Importance . OverallQual | 0.446473 | . GarageCars | 0.135958 | . CentralAir_Y | 0.059187 | . GrLivArea | 0.028329 | . MSZoning_RM | 0.027766 | . KitchenAbvGr | 0.020193 | . TotalBsmtSF | 0.020160 | . RoofMatl_CompShg | 0.017258 | . BsmtQual | 0.016712 | . GarageQual | 0.015863 | . KitchenQual | 0.015859 | . FullBath | 0.011979 | . MSZoning_RL | 0.010424 | . Alley | 0.007786 | . GarageType_Attchd | 0.007374 | . LandSlope | 0.007261 | . Neighborhood_Crawfor | 0.007020 | . Heating_Grav | 0.005074 | . BsmtFinSF1 | 0.004636 | . RoofStyle_Flat | 0.004517 | . ExterQual | 0.004195 | . LandContour_Bnk | 0.004156 | . OverallCond | 0.003864 | . Condition2_Norm | 0.003703 | . MSZoning_C (all) | 0.003549 | . AgeRemod | 0.003455 | . SaleType_WD | 0.003165 | . Condition1_PosA | 0.003120 | . 1stFlrSF | 0.003047 | . Exterior1st_HdBoard | 0.002962 | . HouseStyle_1.5Fin | 0.002960 | . FireplaceQu | 0.002856 | . GarageArea | 0.002603 | . BedroomAbvGr | 0.002332 | . Functional | 0.002304 | . GarageCond | 0.002291 | . Neighborhood_Somerst | 0.002289 | . Exterior1st_BrkFace | 0.002283 | . Age | 0.002091 | . Neighborhood_StoneBr | 0.001960 | . 2ndFlrSF | 0.001952 | . MSZoning_FV | 0.001828 | . LotConfig_CulDSac | 0.001780 | . Neighborhood_ClearCr | 0.001741 | . ExterCond | 0.001660 | . LotArea | 0.001658 | . BsmtFinSF2 | 0.001631 | . Exterior2nd_Wd Shng | 0.001592 | . BsmtExposure | 0.001530 | . Fence_GdPrv | 0.001515 | . Exterior1st_MetalSd | 0.001493 | . TotRmsAbvGrd | 0.001474 | . BsmtFinType1 | 0.001448 | . SaleCondition_Abnorml | 0.001445 | . MSSubClass_class2 | 0.001369 | . PoolArea | 0.001332 | . OpenPorchSF | 0.001320 | . RoofStyle_Gable | 0.001304 | . Neighborhood_Mitchel | 0.001283 | . AgeGarage | 0.001261 | . ScreenPorch | 0.001214 | . BsmtFullBath | 0.001144 | . Condition1_Artery | 0.001122 | . BsmtCond | 0.001035 | . HouseStyle_SLvl | 0.001021 | . Neighborhood_Edwards | 0.000935 | . SaleCondition_Partial | 0.000894 | . Fireplaces | 0.000888 | . LotFrontage | 0.000866 | . Neighborhood_NridgHt | 0.000842 | . Neighborhood_CollgCr | 0.000815 | . PavedDrive | 0.000809 | . LotShape_Reg | 0.000778 | . Neighborhood_Timber | 0.000716 | . Exterior1st_Plywood | 0.000715 | . WoodDeckSF | 0.000711 | . HouseStyle_2Story | 0.000668 | . Neighborhood_SWISU | 0.000665 | . LotConfig_Inside | 0.000644 | . Condition2_Feedr | 0.000625 | . LotShape_IR1 | 0.000583 | . Neighborhood_Sawyer | 0.000576 | . LotConfig_FR2 | 0.000575 | . Electrical_SBrkr | 0.000566 | . RoofStyle_Hip | 0.000565 | . MasVnrType_Stone | 0.000551 | . GarageType_Detchd | 0.000516 | . Exterior1st_Wd Sdng | 0.000513 | . MSSubClass_class1 | 0.000509 | . GarageFinish_Fin | 0.000499 | . Condition1_RRNn | 0.000496 | . Neighborhood_BrkSide | 0.000487 | . EnclosedPorch | 0.000481 | . MSSubClass_class12 | 0.000480 | . GarageFinish_RFn | 0.000473 | . Neighborhood_NAmes | 0.000468 | . Exterior1st_AsbShng | 0.000467 | . Fence_MnPrv | 0.000459 | . MSSubClass_class6 | 0.000458 | . MasVnrArea | 0.000439 | . BsmtUnfSF | 0.000438 | . Neighborhood_Gilbert | 0.000433 | . HeatingQC | 0.000420 | . SaleCondition_AdjLand | 0.000404 | . Condition1_Norm | 0.000394 | . MoSold | 0.000393 | . LotConfig_Corner | 0.000393 | . SaleType_New | 0.000385 | . Foundation_CBlock | 0.000377 | . Condition1_RRAe | 0.000362 | . BsmtHalfBath | 0.000335 | . SaleCondition_Normal | 0.000333 | . Fence_NA | 0.000322 | . Neighborhood_OldTown | 0.000321 | . MasVnrType_None | 0.000312 | . SaleType_ConLI | 0.000299 | . SaleType_COD | 0.000288 | . Neighborhood_NoRidge | 0.000265 | . Exterior1st_Stucco | 0.000259 | . MiscVal | 0.000233 | . Exterior1st_VinylSd | 0.000231 | . Exterior2nd_VinylSd | 0.000221 | . LandContour_Low | 0.000219 | . MSSubClass_class3 | 0.000215 | . LandContour_Lvl | 0.000213 | . HalfBath | 0.000202 | . Fence_GdWo | 0.000201 | . MSSubClass_class9 | 0.000196 | . Neighborhood_NWAmes | 0.000194 | . Exterior2nd_Stucco | 0.000185 | . LotShape_IR2 | 0.000181 | . HouseStyle_1Story | 0.000178 | . RoofMatl_WdShngl | 0.000166 | . GarageType_CarPort | 0.000166 | . MSSubClass_class5 | 0.000163 | . Neighborhood_MeadowV | 0.000159 | . Electrical_FuseA | 0.000158 | . Exterior2nd_HdBoard | 0.000157 | . BldgType_2fmCon | 0.000149 | . 3SsnPorch | 0.000149 | . Exterior2nd_Plywood | 0.000147 | . BsmtFinType2 | 0.000142 | . BldgType_1Fam | 0.000138 | . GarageType_BuiltIn | 0.000125 | . LowQualFinSF | 0.000122 | . Neighborhood_Blmngtn | 0.000106 | . MSSubClass_class7 | 0.000102 | . Exterior2nd_CmentBd | 0.000100 | . MasVnrType_BrkFace | 0.000099 | . Condition1_PosN | 0.000098 | . Neighborhood_SawyerW | 0.000096 | . HouseStyle_SFoyer | 0.000091 | . GarageFinish_Unf | 0.000089 | . Condition1_Feedr | 0.000081 | . Electrical_FuseF | 0.000075 | . Foundation_PConc | 0.000073 | . BldgType_TwnhsE | 0.000070 | . MSSubClass_class10 | 0.000070 | . Neighborhood_BrDale | 0.000067 | . Condition1_RRAn | 0.000064 | . Exterior2nd_ImStucc | 0.000058 | . Neighborhood_IDOTRR | 0.000056 | . Exterior1st_CemntBd | 0.000054 | . Exterior2nd_MetalSd | 0.000045 | . MiscFeature_Othr | 0.000044 | . Exterior1st_WdShing | 0.000043 | . Exterior2nd_Wd Sdng | 0.000042 | . Exterior1st_AsphShn | 0.000040 | . Foundation_Slab | 0.000040 | . Exterior2nd_Stone | 0.000030 | . SaleType_Con | 0.000017 | . Foundation_BrkTil | 0.000010 | . Exterior1st_ImStucc | 0.000004 | . MSSubClass_class11 | 0.000000 | . BldgType_Duplex | 0.000000 | . SaleType_CWD | 0.000000 | . MiscFeature_Gar2 | 0.000000 | . SaleType_ConLD | 0.000000 | . Exterior2nd_Other | 0.000000 | . SaleType_ConLw | 0.000000 | . Foundation_Wood | 0.000000 | . Utilities | 0.000000 | . Street | 0.000000 | . GarageType_Basment | 0.000000 | . PoolQC | 0.000000 | . Neighborhood_NPkVill | 0.000000 | . MSSubClass_class14 | 0.000000 | . MSSubClass_class15 | 0.000000 | . MSSubClass_class16 | 0.000000 | . GarageType_NA | 0.000000 | . Heating_GasA | 0.000000 | . MSSubClass_class4 | 0.000000 | . HouseStyle_2.5Unf | 0.000000 | . HouseStyle_1.5Unf | 0.000000 | . Heating_GasW | 0.000000 | . MSSubClass_class8 | 0.000000 | . MiscFeature_NA | 0.000000 | . MiscFeature_Shed | 0.000000 | . SaleCondition_Alloca | 0.000000 | . Exterior2nd_CBlock | 0.000000 | . Exterior1st_BrkComm | 0.000000 | . Heating_Wall | 0.000000 | . Electrical_FuseP | 0.000000 | . Exterior1st_Stone | 0.000000 | . Exterior2nd_AsbShng | 0.000000 | . Exterior2nd_AsphShn | 0.000000 | . Condition2_RRNn | 0.000000 | . Neighborhood_Veenker | 0.000000 | . Condition2_RRAn | 0.000000 | . RoofMatl_Membran | 0.000000 | . RoofMatl_Metal | 0.000000 | . RoofMatl_Roll | 0.000000 | . RoofMatl_Tar&amp;Grv | 0.000000 | . RoofMatl_WdShake | 0.000000 | . Exterior2nd_Brk Cmn | 0.000000 | . Condition2_PosN | 0.000000 | . Condition2_PosA | 0.000000 | . RoofStyle_Gambrel | 0.000000 | . Condition2_Artery | 0.000000 | . RoofStyle_Mansard | 0.000000 | . Exterior2nd_BrkFace | 0.000000 | . Heating_OthW | 0.000000 | . feat_sel = imp[:56] . feat_list = feat_sel.index.tolist . feat_list() . [&#39;OverallQual&#39;, &#39;GarageCars&#39;, &#39;CentralAir_Y&#39;, &#39;GrLivArea&#39;, &#39;MSZoning_RM&#39;, &#39;KitchenAbvGr&#39;, &#39;TotalBsmtSF&#39;, &#39;RoofMatl_CompShg&#39;, &#39;BsmtQual&#39;, &#39;GarageQual&#39;, &#39;KitchenQual&#39;, &#39;FullBath&#39;, &#39;MSZoning_RL&#39;, &#39;Alley&#39;, &#39;GarageType_Attchd&#39;, &#39;LandSlope&#39;, &#39;Neighborhood_Crawfor&#39;, &#39;Heating_Grav&#39;, &#39;BsmtFinSF1&#39;, &#39;RoofStyle_Flat&#39;, &#39;ExterQual&#39;, &#39;LandContour_Bnk&#39;, &#39;OverallCond&#39;, &#39;Condition2_Norm&#39;, &#39;MSZoning_C (all)&#39;, &#39;AgeRemod&#39;, &#39;SaleType_WD&#39;, &#39;Condition1_PosA&#39;, &#39;1stFlrSF&#39;, &#39;Exterior1st_HdBoard&#39;, &#39;HouseStyle_1.5Fin&#39;, &#39;FireplaceQu&#39;, &#39;GarageArea&#39;, &#39;BedroomAbvGr&#39;, &#39;Functional&#39;, &#39;GarageCond&#39;, &#39;Neighborhood_Somerst&#39;, &#39;Exterior1st_BrkFace&#39;, &#39;Age&#39;, &#39;Neighborhood_StoneBr&#39;, &#39;2ndFlrSF&#39;, &#39;MSZoning_FV&#39;, &#39;LotConfig_CulDSac&#39;, &#39;Neighborhood_ClearCr&#39;, &#39;ExterCond&#39;, &#39;LotArea&#39;, &#39;BsmtFinSF2&#39;, &#39;Exterior2nd_Wd Shng&#39;, &#39;BsmtExposure&#39;, &#39;Fence_GdPrv&#39;, &#39;Exterior1st_MetalSd&#39;, &#39;TotRmsAbvGrd&#39;, &#39;BsmtFinType1&#39;, &#39;SaleCondition_Abnorml&#39;, &#39;MSSubClass_class2&#39;, &#39;PoolArea&#39;] . df_new = df.copy() . df_new = df_new.filter([&#39;OverallQual&#39;, &#39;GarageCars&#39;, &#39;CentralAir_Y&#39;, &#39;GrLivArea&#39;, &#39;MSZoning_RM&#39;, &#39;KitchenAbvGr&#39;, &#39;TotalBsmtSF&#39;, &#39;BsmtQual&#39;, &#39;GarageQual&#39;, &#39;KitchenQual&#39;, &#39;FullBath&#39;, &#39;RoofMatl_CompShg&#39;, &#39;MSZoning_RL&#39;, &#39;Alley&#39;, &#39;GarageType_Attchd&#39;, &#39;LandSlope&#39;, &#39;Neighborhood_Crawfor&#39;, &#39;Condition1_PosA&#39;, &#39;HouseStyle_1.5Fin&#39;, &#39;Heating_Grav&#39;, &#39;BsmtFinSF1&#39;, &#39;RoofStyle_Flat&#39;, &#39;ExterQual&#39;, &#39;OverallCond&#39;, &#39;Condition2_Norm&#39;, &#39;MSZoning_C (all)&#39;, &#39;AgeRemod&#39;, &#39;1stFlrSF&#39;, &#39;Exterior1st_HdBoard&#39;, &#39;FireplaceQu&#39;, &#39;LandContour_Bnk&#39;, &#39;Neighborhood_StoneBr&#39;, &#39;SaleType_WD&#39;, &#39;GarageArea&#39;, &#39;BedroomAbvGr&#39;, &#39;Functional&#39;, &#39;GarageCond&#39;, &#39;Neighborhood_Somerst&#39;, &#39;Exterior1st_BrkFace&#39;, &#39;Age&#39;, &#39;2ndFlrSF&#39;, &#39;MSZoning_FV&#39;, &#39;LotConfig_CulDSac&#39;, &#39;Neighborhood_ClearCr&#39;, &#39;ExterCond&#39;, &#39;LotArea&#39;, &#39;BsmtFinSF2&#39;, &#39;Exterior2nd_Wd Shng&#39;, &#39;BsmtExposure&#39;, &#39;Fence_GdPrv&#39;, &#39;TotRmsAbvGrd&#39;, &#39;BsmtFinType1&#39;, &#39;SaleCondition_Abnorml&#39;, &#39;MSSubClass_class2&#39;, &#39;PoolArea&#39;, &#39;OpenPorchSF&#39;,&#39;SalePrice&#39;]) . X_train = df_new[:-1459].drop([&#39;SalePrice&#39;], axis=1) y_train = df_new[:-1459][&#39;SalePrice&#39;] X_test = df_new[-1459:].drop([&#39;SalePrice&#39;], axis=1) . X_train.shape,y_train.shape,X_test.shape . ((1460, 56), (1460,), (1459, 56)) . xgb = XGBRegressor() xgb.fit(X_train, y_train) . XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=None, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=None, n_estimators=100, n_jobs=0, num_parallel_tree=1, objective=&#39;reg:squarederror&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None, validate_parameters=False, verbosity=None) . xgb.score(X_train, y_train) . 0.9992116339685525 . y_pred = xgb.predict(X_test) . testID = pd.read_csv(&#39;test.csv&#39;) . output = pd.DataFrame({&#39;Id&#39;: testID[&#39;Id&#39;], &#39;SalePrice&#39;: y_pred}) output.to_csv(&#39;predictionfinal.csv&#39;, index=False) . Conclusion . On the Kaggle test data, I put the best-performing model to the test. My model placed me in the top 40% of entrants (at the time of writing). This isn&#39;t a horrible result, but it might be much better. Here are some ideas for how we could go about it: . Categorical variables: The cardinality of several of our categorical characteristics in the data is quite large. As a result, tree models may be skewed toward these characteristics. By reclassifying these high-dimensional features into lower dimensions, we might be able to improve model performance. | . Hyperparameter tuning: We can try to expand our hyperparameter solution space in the hopes of reaching a better position.  | . I hope that this article has given you a better understanding of machine learning. . it&#39;s all about practise and patience. . . twitter: https://twitter.com/jithinharidaas/ .",
            "url": "https://jithinharidas.github.io/paranormal-distributions/2020/08/15/Exploring-HousePrice-Dataset.html",
            "relUrl": "/2020/08/15/Exploring-HousePrice-Dataset.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploring Iris Dataset",
            "content": ". Here we&#39;re exploring basics of classification with the iris data set. . The first step is to use the scikit-learn python package to import the preloaded data sets. . from sklearn.datasets import load_iris #data is saved as a variable iris = load_iris() #view data description and information print(iris.DESCR) . .. _iris_dataset: Iris plants dataset -- **Data Set Characteristics:** :Number of Instances: 150 (50 in each of three classes) :Number of Attributes: 4 numeric, predictive attributes and the class :Attribute Information: - sepal length in cm - sepal width in cm - petal length in cm - petal width in cm - class: - Iris-Setosa - Iris-Versicolour - Iris-Virginica :Summary Statistics: ============== ==== ==== ======= ===== ==================== Min Max Mean SD Class Correlation ============== ==== ==== ======= ===== ==================== sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) ============== ==== ==== ======= ===== ==================== :Missing Attribute Values: None :Class Distribution: 33.3% for each of 3 classes. :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&#39;s paper. Note that it&#39;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points. This is perhaps the best known database to be found in the pattern recognition literature. Fisher&#39;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. .. topic:: References - Fisher, R.A. &#34;The use of multiple measurements in taxonomic problems&#34; Annual Eugenics, 7, Part II, 179-188 (1936); also in &#34;Contributions to Mathematical Statistics&#34; (John Wiley, NY, 1950). - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley &amp; Sons. ISBN 0-471-22361-1. See page 218. - Dasarathy, B.V. (1980) &#34;Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments&#34;. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. - Gates, G.W. (1972) &#34;The Reduced Nearest Neighbor Rule&#34;. IEEE Transactions on Information Theory, May 1972, 431-433. - See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&#34;s AUTOCLASS II conceptual clustering system finds 3 classes in the data. - Many, many more ... . Putting Data into a Data Frame . import pandas as pd data = pd.DataFrame(iris.data) data.head() . 0 1 2 3 . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . renaming the columns for clarity. . data.columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;] data.head() . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . target = pd.DataFrame(iris.target) #rename the column to make it clear that these are the target values target = target.rename(columns = {0: &#39;target&#39;}) target.head() . target . 0 0 | . 1 0 | . 2 0 | . 3 0 | . 4 0 | . The target data frame is only one column, and it gives a list of the values 0, 1, and 2. We will use the information from the feature data to predict if a flower belongs in group 0, 1, or 2. . 0 is Iris Setosa | 1 is Iris Versicolour | 2 is Iris Virginica | . 1. Exploratory Data Analysis (EDA) . df = pd.concat([data, target], axis = 1) . df.head() . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . 1.1 Data Cleaning . It&#39;s critical to go over the data, make sure it&#39;s clean, and then start looking for patterns between characteristics and target variables. . df.dtypes . sepal_length float64 sepal_width float64 petal_length float64 petal_width float64 target int32 dtype: object . float = numbers with decimals | int = integer or whole number without decimals | obj = object, string, or words | . The data types in this data set are all ready to be modelled. . df.isnull().sum() . sepal_length 0 sepal_width 0 petal_length 0 petal_width 0 target 0 dtype: int64 . This data set is not missing any values. . df.describe() . sepal_length sepal_width petal_length petal_width target . count 150.000000 | 150.000000 | 150.000000 | 150.000000 | 150.000000 | . mean 5.843333 | 3.057333 | 3.758000 | 1.199333 | 1.000000 | . std 0.828066 | 0.435866 | 1.765298 | 0.762238 | 0.819232 | . min 4.300000 | 2.000000 | 1.000000 | 0.100000 | 0.000000 | . 25% 5.100000 | 2.800000 | 1.600000 | 0.300000 | 0.000000 | . 50% 5.800000 | 3.000000 | 4.350000 | 1.300000 | 1.000000 | . 75% 6.400000 | 3.300000 | 5.100000 | 1.800000 | 2.000000 | . max 7.900000 | 4.400000 | 6.900000 | 2.500000 | 2.000000 | . 2. Visualizing . import seaborn as sns sns.heatmap(df.corr(), annot = True) . &lt;AxesSubplot:&gt; . The target value is most correlated with the length and width of the petals, which means that as these numbers increase, so does the target value. | In this case, it signifies that flowers in class 2 have petal length and width that are generally longer and wider than flowers in class 0. | Sepal width is the most anti-correlated, implying that flowers in class 0 have the widest sepals compared to flowers in class 2. | also see some intercorrelation between features, for example petal width and length are also highly correlated. | . import matplotlib.pyplot as plt . we can plot scatter plots to further visualize the way the different classes of flowers relate to sepal and petal data. . x_index = 0 y_index = 1 # this formatter will label the colorbar with the correct target names formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)]) plt.figure(figsize=(5, 4)) plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target) plt.colorbar(ticks=[0, 1, 2], format=formatter) plt.xlabel(iris.feature_names[x_index]) plt.ylabel(iris.feature_names[y_index]) plt.tight_layout() plt.show() . Now let&#8217;s create the same scatter plot to compare the petal data points. . x_index = 2 y_index = 3 # this formatter will label the colorbar with the correct target names formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)]) plt.figure(figsize=(5, 4)) plt.scatter(iris.data[:, x_index], iris.data[:, y_index], c=iris.target) plt.colorbar(ticks=[0, 1, 2], format=formatter) plt.xlabel(iris.feature_names[x_index]) plt.ylabel(iris.feature_names[y_index]) plt.tight_layout() plt.show() . 3. Modeling . X = df.copy() y = X.pop(&#39;target&#39;) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=1, stratify = y) . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns) X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns) . 3.1 Baseline Prediction . The baseline is the probability of predicting class before the model is implemented. If the data is split into 2 classes evenly, there is already a 50% chance of randomly assigning an element to the correct class. The goal of our model is to improve on this baseline, or random prediction. Also, if there is a strong class imbalance (if 90% of the data was in class 1), then we could alter the proportion of each class to help the model predict more accurately. . df.target.value_counts(normalize= True) . 0 0.333333 1 0.333333 2 0.333333 Name: target, dtype: float64 . The baseline prediction for this model is 1/3 . 3.2 Logistic Regression Model . import numpy as np . from sklearn.linear_model import LogisticRegression #create the model instance model = LogisticRegression() #fit the model on the training data model.fit(X_train, y_train) #the score, or accuracy of the model model.score(X_test, y_test) . 0.9666666666666667 . from sklearn.model_selection import cross_val_score scores = cross_val_score(model, X_train, y_train, cv=10) print(np.mean(scores)) . 0.9499999999999998 . Without any adjustments or tuning, this model is already performing very well with a test score of .9667 and a cross validation score of .9499. This means that the model is predicting the correct class for the flower about 95% of time. Much higher than the baseline of 33%! . 4. Understanding the Predictions . df_coef = pd.DataFrame(model.coef_, columns=X_train.columns) df_coef . sepal_length sepal_width petal_length petal_width . 0 -1.102746 | 1.001818 | -1.836891 | -1.667978 | . 1 0.402982 | -0.323432 | -0.277761 | -0.650011 | . 2 0.699764 | -0.678386 | 2.114653 | 2.317989 | . Coefficients are often a bit hard to interpret in Logistic Regression, but we can get an idea of how much of an impact each of the features had in deciding if a flower belonged to that class. For instance, petal length was barely a deciding factor for if a flower was in class 1, but petal width was a strong predictor for class 2 . predictions = model.predict(X_test) #compare predicted values with the actual scores compare_df = pd.DataFrame({&#39;actual&#39;: y_test, &#39;predicted&#39;: predictions}) compare_df = compare_df.reset_index(drop = True) compare_df . actual predicted . 0 2 | 2 | . 1 0 | 0 | . 2 1 | 1 | . 3 0 | 0 | . 4 0 | 0 | . 5 0 | 0 | . 6 2 | 2 | . 7 2 | 2 | . 8 2 | 2 | . 9 1 | 1 | . 10 0 | 0 | . 11 1 | 1 | . 12 2 | 2 | . 13 1 | 1 | . 14 2 | 2 | . 15 0 | 0 | . 16 2 | 2 | . 17 1 | 1 | . 18 1 | 1 | . 19 2 | 2 | . 20 1 | 1 | . 21 1 | 1 | . 22 0 | 0 | . 23 0 | 0 | . 24 2 | 2 | . 25 2 | 1 | . 26 0 | 0 | . 27 0 | 0 | . 28 1 | 1 | . 29 1 | 1 | . The predictions line up almost perfectly, and only once the model incorrectly predicted that a flower belonged to class 1 when it really belonged to class 2. . from sklearn.metrics import confusion_matrix pd.DataFrame(confusion_matrix(y_test, predictions, labels=[2, 1, 0]),index=[2, 1, 0], columns=[2, 1, 0]) . 2 1 0 . 2 9 | 1 | 0 | . 1 0 | 10 | 0 | . 0 0 | 0 | 10 | . We can see that class 0 and 1 were all predicted correctly all 10 times, but the model incorrectly labeled class 2 as class 1 in one instance. . Precision: Number of correctly predicted Iris Virginica flowers (10) out of total number of predicted Iris Virginica flowers (10). Precision in predicting Iris Virginica =10/10 = 1.0 | Recall: Number of correctly predicted Iris Virginica out of the number of actual Iris Virginica. Recall = 9/10 = .9 | F1 Score: This is a harmonic mean of precision and recall. The formula is F1 Score = 2 (precision recall) / (precision + recall) | Accuracy: Add all the correct predictions together for all classes and divide by the total number of predictions. 29 correct predictions /30 total values = accuracy of .9667. | . from sklearn.metrics import classification_report print(classification_report(y_test, predictions)) . precision recall f1-score support 0 1.00 1.00 1.00 10 1 0.91 1.00 0.95 10 2 1.00 0.90 0.95 10 accuracy 0.97 30 macro avg 0.97 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 . probs = model.predict_proba(X_test) #put the probabilities into a dataframe for easier viewing Y_pp = pd.DataFrame(model.predict_proba(X_test), columns=[&#39;class_0_pp&#39;, &#39;class_1_pp&#39;, &#39;class_2_pp&#39;]) Y_pp.head() . class_0_pp class_1_pp class_2_pp . 0 0.000016 | 0.062182 | 9.378022e-01 | . 1 0.958819 | 0.041181 | 5.060799e-07 | . 2 0.147033 | 0.846368 | 6.598360e-03 | . 3 0.983033 | 0.016967 | 2.623772e-07 | . 4 0.970334 | 0.029663 | 2.118099e-06 | . 5. Conclusion . This is a typical data set since it is simple to work with, but the steps outlined here may be applied to any classification project. . . twitter: https://twitter.com/jithinharidaas/ .",
            "url": "https://jithinharidas.github.io/paranormal-distributions/2020/06/21/Exploring-Iris-Dataset.html",
            "relUrl": "/2020/06/21/Exploring-Iris-Dataset.html",
            "date": " • Jun 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Jithin K Haridas . . I specialise in Advanced Analytics, a combination of exploratory data analysis, prescriptive and predictive analytics leveraging techniques across areas like statistical modelling, data mining, machine learning, text analytics. . .",
          "url": "https://jithinharidas.github.io/paranormal-distributions/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jithinharidas.github.io/paranormal-distributions/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}